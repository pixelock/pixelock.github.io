<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>LLaMA on 外置记忆体</title>
        <link>http://localhost:1313/tags/llama/</link>
        <description>Recent content in LLaMA on 外置记忆体</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>zh-cn</language>
        <copyright>pixelock</copyright>
        <lastBuildDate>Mon, 13 May 2024 23:11:05 +0800</lastBuildDate><atom:link href="http://localhost:1313/tags/llama/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>Llama 2 Insight</title>
        <link>http://localhost:1313/p/llama-2-insight/</link>
        <pubDate>Mon, 13 May 2024 23:11:05 +0800</pubDate>
        
        <guid>http://localhost:1313/p/llama-2-insight/</guid>
        <description>&lt;p&gt;&lt;img src=&#34;https://cdn.jsdelivr.net/gh/pixelock/notebook-images/images/20240422171402.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;h1 id=&#34;pre-training&#34;&gt;Pre-training
&lt;/h1&gt;&lt;h2 id=&#34;数据&#34;&gt;数据
&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;对数据源进行了限制, 只从具有很高真实性的数据源中获取数据, 并进行 up-sampling, 增强知识, 抑制幻觉&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;数据规模&#34;&gt;数据规模
&lt;/h3&gt;&lt;p&gt;2T tokens.&lt;/p&gt;
&lt;h2 id=&#34;训练细节&#34;&gt;训练细节
&lt;/h2&gt;&lt;h3 id=&#34;模型结构细节&#34;&gt;模型结构细节
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;Pre-Norm with RMSNorm&lt;/li&gt;
&lt;li&gt;SwiGLU activation function&lt;/li&gt;
&lt;li&gt;RoPE&lt;/li&gt;
&lt;li&gt;GQA, grouped-query attention&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;训练参数&#34;&gt;训练参数
&lt;/h3&gt;&lt;p&gt;训练使用的 learning rate 和 context length 因模型大小而异, 详情见下图&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;优化器 AdamW, $\beta_{1}=0.9$, $\beta_{2}=0.95$, $\varepsilon=10^{-5}$, $\text{weight decay}=0.1$&lt;/li&gt;
&lt;li&gt;Warmup: 2000 steps&lt;/li&gt;
&lt;li&gt;Cosine learning rate schedule, 最终学习率衰减到最大学习率的 10%&lt;/li&gt;
&lt;li&gt;Gradient clipping: 0.1&lt;/li&gt;
&lt;li&gt;Learning rate:
&lt;ul&gt;
&lt;li&gt;7B: $3 \times 10^{-4}$&lt;/li&gt;
&lt;li&gt;13B: $3 \times 10^{-4}$&lt;/li&gt;
&lt;li&gt;34B: $1.5 \times 10^{-4}$&lt;/li&gt;
&lt;li&gt;70B: $1.5 \times 10^{-4}$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;词表大小: 32k&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://cdn.jsdelivr.net/gh/pixelock/notebook-images/images/20240428182510.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;最终损失降低到:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;7B: 1.75&lt;/li&gt;
&lt;li&gt;13B: 1.77&lt;/li&gt;
&lt;li&gt;34B: 1.57&lt;/li&gt;
&lt;li&gt;70B: 1.50&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;sft&#34;&gt;SFT
&lt;/h1&gt;&lt;h2 id=&#34;数据收集&#34;&gt;数据收集
&lt;/h2&gt;&lt;p&gt;人工编写 Prompt + Answer, 收集了 27540 高质量的 SFT data. 高质量数据包括两大类:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;helpfulness. 样本的 response 确实可以解决 prompt 的任务&lt;/li&gt;
&lt;li&gt;safety. 对于不安全的 prompt 拒绝回答&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Meta 在 SFT 这一步, 只收集了 20k+ 量级的. 做出这个决策的原因是, 在使用这个量级的数据 SFT 之后, 模型的输出, 与人类标注的质量可以相比较. 因此团队认为 SFT 的标注工作可以结束, 将标注资源放在 RLHF 要使用的偏好数据的标注.&lt;/p&gt;
&lt;h2 id=&#34;训练参数-1&#34;&gt;训练参数
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;Batch Size: 64&lt;/li&gt;
&lt;li&gt;Learning Rate: 2e-5&lt;/li&gt;
&lt;li&gt;Learning Rate Schedule: cosine learning rate schedule&lt;/li&gt;
&lt;li&gt;Sequence Length: 4096&lt;/li&gt;
&lt;li&gt;Weight decay: 0.1&lt;/li&gt;
&lt;li&gt;Epochs: 2&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;使用了 Packing 策略, 将训练集中所有的 prompts 和 answers 连接在一起后按长度切分, 保证序列长度被完全使用. 使用一个特殊符号作为 prompt 和 answer 的分隔.&lt;/p&gt;
&lt;h1 id=&#34;rlhf&#34;&gt;RLHF
&lt;/h1&gt;&lt;p&gt;RLHF 的目标是将模型的输出行为对齐于人类偏好(human preferences)和遵循指令(instruction following).&lt;/p&gt;
&lt;h2 id=&#34;收集人类偏好数据&#34;&gt;收集人类偏好数据
&lt;/h2&gt;&lt;p&gt;收集人类偏好数据(human preference data)来训练奖励模型. 收集的方法如下:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;人工编写 prompt&lt;/li&gt;
&lt;li&gt;将编写的 prompt 输入到 SFT 后的模型中, 得到两个输出作为采样, 并为这两个采样标注哪个回答更好&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;为了让采样更具有多样性, 使用同一个 prompt 采样时, 使用不同的模型(model variants)进行采样(猜测是训练了两个 SFT 模型), 并且使用了不同的 temperature.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;为偏好划分了 4 种标签: &lt;code&gt;significantly better&lt;/code&gt;, &lt;code&gt;better&lt;/code&gt;, &lt;code&gt;slightly better&lt;/code&gt;, &lt;code&gt;negligibly better/ unsure&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;偏好标注的关注点, 在与回答的 &lt;strong&gt;有用性(helpfulness)&lt;/strong&gt; 和 &lt;strong&gt;安全性(safety)&lt;/strong&gt; 两个方面, 因此判断四种标签的方法为:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;helpfulness&lt;/strong&gt;: &lt;code&gt;LLaMA2-Chat&lt;/code&gt; 的回答可以满足用户要求, 提供所需的信息&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;safety&lt;/strong&gt;: 模型的回答是否是安全的, 标签被设计为 3 类:
&lt;ul&gt;
&lt;li&gt;选择的回答更安全, 另外的回答不安全. 最终占整个数据集的 18%&lt;/li&gt;
&lt;li&gt;两个回答都是安全的. 47%&lt;/li&gt;
&lt;li&gt;两个回答都是不安全的. 35%&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;两者的标注是分开的. 例如 &lt;code&gt;giving detailed instructions on making a bomb&lt;/code&gt; 的回答可以被认为有用, 但是不安全的. 这种分开标注, 相互不纠缠, 有更清晰的标注引导, 标注的质量会更高.&lt;/p&gt;
&lt;p&gt;从 safety 的三类标签也能看到, 抛弃了 &lt;em&gt;选择的回答是不安全, 另外的回答是安全的&lt;/em&gt; 这种情况, 因为安全的回答才有资格作为更好的答案.&lt;/p&gt;
&lt;p&gt;最终收集了 1,418,091 条人工偏好数据.&lt;/p&gt;
&lt;h3 id=&#34;safety-偏好数据&#34;&gt;Safety 偏好数据
&lt;/h3&gt;&lt;p&gt;safety 方面, 针对性地编写了一些 &lt;strong&gt;对抗性的 prompt(adversarial prompts)&lt;/strong&gt;, 从两个角度进行了编写.&lt;/p&gt;
&lt;h4 id=&#34;risk-category&#34;&gt;Risk category
&lt;/h4&gt;&lt;p&gt;Risk category, 可以理解为是可能产生不安全内容的潜在主题(topic). LLaMA 2 中划分了三个类别:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;illicit and criminal activities&lt;/strong&gt;: 各种犯罪行为&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;hateful and harmful activities&lt;/strong&gt;: 歧视, 诽谤, 自残等行为&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;unqualified advice&lt;/strong&gt;: 例如医疗建议, 金融建议, 法律建议等各种严肃建议的场景&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;attack-vectors&#34;&gt;Attack vectors
&lt;/h4&gt;&lt;p&gt;Attack vectors 可以理解为 prompt 的多种提问风格, 这种风格可以诱发模型做出不好的回答. 考虑了以下几种:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;psychological manipulation&lt;/strong&gt;: 心理操纵&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;logic manipulation&lt;/strong&gt;: 逻辑操纵, 如虚假假设&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;syntactic manipulation&lt;/strong&gt;: 句法操纵, 如故意的错误拼写, 汉语中还有形近字, 音近字, 拆字等攻击&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;semantic manipulation&lt;/strong&gt;: 语义操控, 如隐喻, 阴阳怪气..&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;perspective manipulation&lt;/strong&gt;: 透视操纵, 如不合适的角色扮演&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;收集-训练迭代&#34;&gt;收集-训练迭代
&lt;/h3&gt;&lt;p&gt;定期收集人工标注数据, LLaMA 2 中每周收集一次.&lt;/p&gt;
&lt;p&gt;在收集到更多的人工偏好数据后, 训练得到更好的奖励模型, 再通过 PPO 训练, 得到更好的 Chat 模型.&lt;/p&gt;
&lt;p&gt;在得到更好的 Chat 模型之后, 从 Chat 模型中采样得到的数据分布也会发生变化, 这会让模型产生一些新的数据分布. 这些新数据分布, 在下轮的训练中, 会拓宽奖励模型的视野, 提高模型的泛化性和整体性能.&lt;/p&gt;
&lt;p&gt;这种一轮轮迭代的方式, 帮助奖励模型的分布不断拓宽, 进而经过 RLHF 的 Chat 模型也进一步提升.&lt;/p&gt;
&lt;h2 id=&#34;reward-modeling&#34;&gt;Reward Modeling
&lt;/h2&gt;&lt;p&gt;有研究发现 helpfulness 和 safety 存在 trade off 的情况, 这会使得用同一个奖励模型, 在这两个评价任务中都得到好的效果, 是非常有挑战性的. 因此, LLaMA 2 分开训练了两个奖励模型: Helpfulness RM 和 Safety RM.&lt;/p&gt;
&lt;p&gt;奖励模型使用&lt;strong&gt;预训练模型&lt;/strong&gt;的 checkpoints 作为初始化, 保证模型具备预训练过程中获取的知识, 防止出现奖励模型和 RLHF 训练目标的 Chat 模型, 出现知识不匹配的情况, 进而导致产生幻觉.&lt;/p&gt;
&lt;h3 id=&#34;训练目标&#34;&gt;训练目标
&lt;/h3&gt;&lt;p&gt;使用 &lt;strong&gt;Binary ranking loss&lt;/strong&gt;, 目标是让 RM 对偏好的回复产生更高的分数.&lt;/p&gt;
&lt;p&gt;$$
\mathcal{L}_{\text {ranking }}=-\log \left(\sigma\left(r_\theta\left(x, y_c\right)-r_\theta\left(x, y_r\right)-m(r)\right)\right)
$$&lt;/p&gt;
&lt;p&gt;$r_\theta\left(x, y_c\right)$ 代表 Prompt $x$ 和 Chat 模型的 completion $y$ 给到 RM 模型 $\theta$ 得到的标量分数. $y_{c}$ 代表是标注人员更偏好的回复, $y_{r}$ 代表的是被拒绝的回复.&lt;/p&gt;
&lt;p&gt;Meta 进一步引入了 &lt;strong&gt;margin&lt;/strong&gt; 成分 $m(r)$. Binary ranking loss 是减函数, 两个 completion 之间的 score 差距越大, 对应的损失就越小, 这也符合直觉. 而在融入减去一个非负的 margin 成分之后, 缩小了 completion 之间的 score 差别, 产生更大的损失, 迫使模型将两者之间的距离拉的更远.&lt;/p&gt;
&lt;p&gt;$m(r)$ 是一个离散函数, 它利用了偏好数据的 4 种标签: &lt;code&gt;significantly better&lt;/code&gt;, &lt;code&gt;better&lt;/code&gt;, &lt;code&gt;slightly better&lt;/code&gt;, &lt;code&gt;negligibly better/ unsure&lt;/code&gt;, 显式地利用这些标签来对不同差别的 completion 施加不同的 margin 大小, 整体上是对差异更显著的 completion 增加更大的 margin, 以期望拉开更大的差距.&lt;/p&gt;
&lt;p&gt;Meta 实验了两套大小不同的 $m(r)$ 离散函数 , 对应于下表中的 &lt;code&gt;Margin Small&lt;/code&gt; 和 &lt;code&gt;Margin Large&lt;/code&gt;, 实验证明增加的 margin 项确实提升 RM 的性能, 且更大的尺度的 $m(r)$ 对应的效果更好, 但更大尺度的 $m(r)$ 对相近回答的性能有所降低.&lt;/p&gt;
&lt;p&gt;$m(r)$ 离散函数详情:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cdn.jsdelivr.net/gh/pixelock/notebook-images/images/20240430142914.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;实验结果&lt;/strong&gt;, 数值代表的是准确率:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cdn.jsdelivr.net/gh/pixelock/notebook-images/images/20240430143000.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;训练数据&#34;&gt;训练数据
&lt;/h3&gt;&lt;p&gt;训练 RM 的数据集, 由上面所收集的人工偏好数据和开源偏好数据集组合而成.&lt;/p&gt;
&lt;p&gt;初始时, 由于没有人工数据, 使用开源偏好数据训练得到初版 RM 模型, 同时并行地收集人工偏好数据. 这里有一个矛盾点, RLHF 需要的 reward single, 应当是对 LLaMA 2-Chat 模型的输出进行的学习, 而开源数据是其他模型产生的. 但在论文的实验中, 没有观察到开源数据带来任何的负迁移效果. 因此最终的数据集混合了这些开源数据, 而这些开源数据, 会为 RM 带来更好的泛化性能.&lt;/p&gt;
&lt;p&gt;对于生成数据和开源数据这两种数据集, Helpfulness RM 和 Safety RM, 使用了不同的最佳混合比例&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Helpfulness RM 的训练集
&lt;ul&gt;
&lt;li&gt;使用了全部的 &lt;code&gt;人工标注得到的 Helpfulness 数据&lt;/code&gt;, 占数据集的一半&lt;/li&gt;
&lt;li&gt;另外一半是从 &lt;code&gt;人工标注得到的 Safety 数据&lt;/code&gt; 和 &lt;code&gt;开源数据集&lt;/code&gt; 中采样得到&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Safety RM 的训练集
&lt;ul&gt;
&lt;li&gt;使用了全部的 &lt;code&gt;人工标注得到的 Safety 数据&lt;/code&gt;, 以及 &lt;code&gt;Anthropic Harmless data&lt;/code&gt;, 这部分占整体的 90%&lt;/li&gt;
&lt;li&gt;另外 10% 混合了 &lt;code&gt;人工标注得到的 Helpfulness 数据&lt;/code&gt; 和 &lt;code&gt;开源的 helpfulness 数据集&lt;/code&gt;. 混合了这 10% 的 helpfulness 数据集之后, 能够有效的提升 Safety RM 在 chosen 和 rejected 都是安全的样本中的准确率&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;训练参数-2&#34;&gt;训练参数
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;Epoch: 1, 训练的步数多会导致 over-fitting&lt;/li&gt;
&lt;li&gt;Learning Rate
&lt;ul&gt;
&lt;li&gt;5e-6 for 70B&lt;/li&gt;
&lt;li&gt;1e-5 for other scales&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Learning Rate Schedule: cosine learning rate schedule, 最低降到 learning rate 的 10%&lt;/li&gt;
&lt;li&gt;Warm-up: 3% total steps&lt;/li&gt;
&lt;li&gt;Batch size: 512 pairs per batch&lt;/li&gt;
&lt;li&gt;Weight decay: 0.1&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;强化学习&#34;&gt;强化学习
&lt;/h2&gt;&lt;h3 id=&#34;训练策略&#34;&gt;训练策略
&lt;/h3&gt;&lt;p&gt;Meta 探索了两种 RLHF 中常用的两种微调算法:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;PPO(Proximal Policy Optimization)&lt;/li&gt;
&lt;li&gt;拒绝采样微调(Rejection Sampling fine-tuning)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;这两种 RL 算法的区别在于:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;拒绝采样对于每个 prompt, 采样出 K 个样本; PPO 只采样一个样本&lt;/li&gt;
&lt;li&gt;PPO 在每一步 policy 模型参数更新后, 对当前训练的 prompt 进行采样; 拒绝采样时在强化学习开始之前, 从初始的 policy 中对&lt;strong&gt;所有&lt;/strong&gt;的 prompt 都进行采样, 一次性采样得到所有输出, 这个其实就是 SFT&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Meta 在强化学习这一步, 使用迭代训练的策略. 由于 RM 使用的偏好训练集是一批批采样标注得到的, 使用新标注的数据得到更好的 RM 模型, 并获取更多的 prompts, 这是训练更好的 RLHF 模型的基础. 实际上, Meta 训练了 &lt;code&gt;RLHF-V1&lt;/code&gt; 到 &lt;code&gt;RLHF-V5&lt;/code&gt; 共 5 个版本的模型.&lt;/p&gt;
&lt;p&gt;在包括 &lt;code&gt;RLHF-V4&lt;/code&gt; 在内的早期版本中, 只使用&lt;strong&gt;拒绝采样&lt;/strong&gt;进行训练. 在 &lt;code&gt;V5&lt;/code&gt; 中, 顺序的使用这两种方法, 先使用拒绝采样方法训练, 再挑选出 Evaluation 最高的 checkpoint, 使用 PPO 继续进行训练.&lt;/p&gt;
&lt;h3 id=&#34;拒绝采样&#34;&gt;拒绝采样
&lt;/h3&gt;&lt;h4 id=&#34;拒绝采样的做法&#34;&gt;拒绝采样的做法
&lt;/h4&gt;&lt;p&gt;拒绝采样本质上就是在进行 SFT, 只是用来训练的样本是从模型中采样得到的.&lt;/p&gt;
&lt;p&gt;拒绝采样这种 RL 方法首先在 70B 规模的 LLaMA2-Chat 模型上进行, 采样得到的样本, 除了用来训练 70B 的模型, 所有更小规模的模型也是用这些数据做拒绝采样的训练, 而不是各个规模的模型各自自己采样. 这样做的目的, 是将大模型的能力&lt;strong&gt;蒸馏&lt;/strong&gt;到小模型中去.&lt;/p&gt;
&lt;p&gt;RLHF 共经过了 &lt;code&gt;RLHF-V1&lt;/code&gt; 到 &lt;code&gt;RLHF-V5&lt;/code&gt; 5 个阶段, 每个阶段的训练中, 对于每个 prompt 样本, 使用上个阶段得到的样本采样 K 个 answers, 并选择当前最优的 RM 进行评价, 得到分数最高的样本. 在早期的探索中, 训练当前阶段使用的样本, 都是用上个阶段的模型采样得到, 例如训练 &lt;code&gt;RLHF-V3&lt;/code&gt; 使用的样本全部来自 &lt;code&gt;RLHF-V2&lt;/code&gt; 的采样. 但这种方法在整体指标提升的同时, 会导致某些方面能力的退化. 例如通过这种方法训练得到的 &lt;code&gt;RLHF-V3&lt;/code&gt; 在编写押韵诗句方面比之前的版本更差.&lt;/p&gt;
&lt;p&gt;为了解决这个问题, 每个阶段的训练, 会使用之前所有阶段产生的样本作为候选池, 从中选出 score 最高的一批样本作为训练数据集. 例如, 训练 &lt;code&gt;RLHF-V3&lt;/code&gt; 会使用 &lt;code&gt;RLHF-V2&lt;/code&gt; 和 &lt;code&gt;RLHF-V1&lt;/code&gt; 的样本.&lt;/p&gt;
&lt;h4 id=&#34;拒绝采样为什么能起作用&#34;&gt;拒绝采样为什么能起作用
&lt;/h4&gt;&lt;p&gt;为什么这种 SFT 的形式可以用作 RLHF 呢?&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cdn.jsdelivr.net/gh/pixelock/notebook-images/images/20240515225919.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;上图呈现了单个 prompt 随着采样数量 N 的增加, 这 N 个采样 reward score 的最大值和中位数的变化情况. 每个点的值是统计训练集中所有 prompt 值的平均得到. 拒绝采样用的是 N 个采样中 score 最大的样本作为训练样本, 对应着图中的蓝线. 而橙色线代表着一次采样的期望值, 因此最大值和中间值之间的 delta 就代表了拒绝采样使用最优采样做 fine-tuning 的潜在收益.&lt;/p&gt;
&lt;p&gt;随着单个 prompt 采样量 N 的增加, 这个潜在收益还会逐渐增大, 或者说探索的广度越大, 收益也会越大, 是一种用广度换深度(PPO每次采样后更新模型, 再采样循环的方法可以看做是在深度上进行探索)的策略.&lt;/p&gt;
&lt;p&gt;因此温度这个超参数对探索也会起到至关重要的作用, 更高的温度能够让采样结果的多样性增加, 但也不是用最大温度就可以.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cdn.jsdelivr.net/gh/pixelock/notebook-images/images/20240515232434.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;上图反应了 LLaMA2-Chat-SFT 和 LLaMA2-Chat-RLHF 两个模型采样 N 个样本对应的最大 reward score 的曲线, 每条曲线代表着一个采样温度. 可以观察到, 采样的最优温度, 在迭代训练的过程中是在变化的.&lt;/p&gt;
&lt;p&gt;因此在每个阶段的 RLHF 模型训练过程中, 每训练一定的步数, 就要暂停下来, 在不同的温度下采样一批, 根据 max reward score 评估当前阶段的最优采样温度.&lt;/p&gt;
&lt;h4 id=&#34;类似的做法&#34;&gt;类似的做法
&lt;/h4&gt;&lt;p&gt;LLaMA2 中使用的拒绝采样方法, 与 &lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2304.06767&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;RAFT: Reward rAnked FineTuning for Generative Foundation Model Alignment&lt;/a&gt; 中提到的方法完全相同.&lt;/p&gt;
&lt;p&gt;另外还有 &lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2304.05302&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;RRHF: Rank Responses to Align Language Models with Human Feedback without tears&lt;/a&gt; 方法. 对某一个 prompt, 通过不同的方式(LLM 采样, 人类专家等)生成很多个 answers, 让 RM 去打分, 然后选出好的去 Fine-tune 大模型. 此外, 让大模型也算一下这些 answers 的似然概率当做评分, 让这个评分和RM的打分尽可能对齐, 这个对齐通过设置 Rank loss 进行监督学习来实现.&lt;/p&gt;
&lt;h3 id=&#34;ppo&#34;&gt;PPO
&lt;/h3&gt;&lt;p&gt;PPO 中的 reward function 如下. 这里由于有 Helpfulness RM 和 Safety RM 两种, 所以 reward function 中的 RM score 部分, 需要混合这两个模型的输出.&lt;/p&gt;
&lt;p&gt;$$
R(g \mid p)=\bar{R}_c(g \mid p)-\beta D_{K L}\left(\pi_\theta(g \mid p) | \pi_0(g \mid p)\right)
$$&lt;/p&gt;
&lt;p&gt;其中 $\bar{R}_c(g \mid p)$ 是混合后 RM 的输出, 它的定义如下:&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
&amp;amp; R_c(g \mid p)= \begin{cases}R_s(g \mid p) &amp;amp; \text{if IS\_SAFETY}(p) \text { or } R_s(g \mid p)&amp;lt;0.15 \\
R_h(g \mid p) &amp;amp; \text { otherwise }\end{cases}
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;$$
\tilde{R}_c(g \mid p)=\operatorname{WHITEN}\left(\operatorname{LOGIT}\left(R_c(g \mid p)\right)\right)
$$&lt;/p&gt;
&lt;p&gt;其中 $R_s(g \mid p)$ 代表 Safety RM 的输出, $R_h(g \mid p)$ 代表 Helpfulness RM 的输出. 优先考虑安全性. 其中人工编写的 prompt 已经标记了哪些是可能引发不安全回答的, 对应上式中的 $\text{IS\_SAFETY}(p)$, 对于这部分可能引发安全问题的样本, 以及 Safety RM 输出的分数小于 0.15 的阈值的不安全回答, 这两类的不安全情况, 优先考虑安全分数. 0.15 阈值对应的是 Safety RM 0.89 的准确率和在 Meta Safety test set 上 0.55 的召回率.&lt;/p&gt;
&lt;p&gt;其他安全的情况, 再考虑 Helpfulness RM 的输出.&lt;/p&gt;
&lt;p&gt;在混合了两种 RM 的输出后, $\text{LOGIT}$ 是 sigmoid 的反函数, 再进行白化, 目的是增加稳定性, 与 KL 散度损失项取得合适的平衡.&lt;/p&gt;
&lt;h3 id=&#34;训练参数-3&#34;&gt;训练参数
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;使用 AdamW 优化器, 对应的参数为 $\beta_{1}=0.9$, $\beta_{2}=0.95$, $\varepsilon=10^{-5}$, $\text{weight decay}=0.1$&lt;/li&gt;
&lt;li&gt;Gradient clipping 1.0&lt;/li&gt;
&lt;li&gt;Learning rate: 1e-6&lt;/li&gt;
&lt;li&gt;PPO Micro batch size 64, batch size 512, PPO clip threshold 0.2&lt;/li&gt;
&lt;li&gt;损失函数中的 KL 系数, 在 7B 和 13B 模型中 $\beta=0.01$, 在 34B 和 70B 模型中 $\beta=0.005$&lt;/li&gt;
&lt;li&gt;不同规模的模型训练步数在 200 到 400 步之间, 并构建了 dev prompts 数据集, 做 early stopping&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;multi-turn-一致性--ghost-attention&#34;&gt;Multi-Turn 一致性 / Ghost Attention
&lt;/h1&gt;&lt;p&gt;在对话过程中, 往往会有一个 System Message, 比如要求回复简洁, 或者要求 assistant 扮演一个角色. 期望是这个 System Message, 在整个对话过程中持续作用.&lt;/p&gt;
&lt;p&gt;但在最初的 RLHF 迭代中, 得到的模型在经过几轮的交谈后, 就倾向于遗忘最初的 system message 中的要求了.&lt;/p&gt;
&lt;p&gt;为了解决这个问题, 提出了 Ghost Attention(GAtt). 这不是一种模型结构, 而是一种调整 fine-tuning 数据的方式.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Ghost Attention&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;把一个 $n$ 轮的多轮对话记为 $\left[ u_1,a_1,\cdots,u_n,a_n \right]$, 把 system message 定义为 $\text{inst}$. 接下来, 将 $\text{inst}$ 与每轮 user message $u_i$ 拼接起来.&lt;/p&gt;
&lt;p&gt;然后使用最新的 RLHF 模型, 对这个多轮对话中每个拼接后的 user message 进行多次采样, 使用 RM 选择最好作为训练样本(拒绝采样的方法). 这样就得到了符合指令的多轮对话数据.&lt;/p&gt;
&lt;p&gt;再将除了第一轮之外的所有 user message 中的 $\text{inst}$ 部分删除, 用这个数据进行 fine-tuning.&lt;/p&gt;
&lt;p&gt;在训练时, 为了实现多轮以后的 system 指令遵从能力, 将多轮数据中前面几轮中所有 tokens 的 loss 置为 0, 包括原本要计算 loss 的 assistant message 部分.&lt;/p&gt;
&lt;p&gt;以上就是 Ghost Attention 的做法.&lt;/p&gt;
&lt;p&gt;论文中 system message 从一些任务中合成得到:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Hobbies, 例如 &lt;code&gt;You enjoy e.g. Tennis&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Language, 例如 &lt;code&gt;Speak in e.g. French&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Public Figure, 例如 &lt;code&gt;Act as e.g. Napoleon&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;而 Hobbies 和 Public Figure 合成用到的 list, 是通过 LLaMA2-Chat 生成, 以防止出现模型知识不具备的情况, 会加重幻觉. 而为了让 system instruction 更复杂以及更有多样性, 最终的 instruction 是随机地将上面的几类组合得到. 同时还会将 instruction 进行简化, 例如 &lt;code&gt;Always act as Napoleon from now -&amp;gt; Figure: Napoleon&lt;/code&gt;. 使用这些方法, 生成了一个 SFT 数据集, 用这个数据集微调出 LLaMA2-Chat.&lt;/p&gt;
&lt;h1 id=&#34;总结&#34;&gt;总结
&lt;/h1&gt;&lt;p&gt;&lt;strong&gt;SFT的必要性和问题&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;每个标注人员的创作的 prompt 和 completion 都有很大的多样性, 在这些数据集上进行 SFT, 得到的模型可以学习这种多样性.&lt;/p&gt;
&lt;p&gt;这种多样性是很长尾的, 这些长尾数据中也会有很多不好的结果, 这些是 SFT 无法解决的.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;RLHF的作用&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;比较两个 completion 哪个更好, 这个任务相对更简单, 因此 RM 可以很快地学习到将低分分配给效果不好的长尾分布, 使得最差的答案在分布上逐渐被删除. 这一点是通过人类偏好数据标注和 RLHF 训练过程配合得到的.&lt;/p&gt;
&lt;p&gt;LLM 超越人类能力的上限, 是通过 RLHF 的人类监督信号得到的, 它比 SFT 更重要.&lt;/p&gt;
</description>
        </item>
        
    </channel>
</rss>
