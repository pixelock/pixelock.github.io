[{"content":"模型权重\n几种模型权重:\nliuhaotian/llava-v1.6-mistral-7b LLM 基于 mistralai/Mistral-7B-Instruct-v0.2 视觉侧基于 openai/clip-vit-large-patch14-336 liuhaotian/llava-v1.6-34b LLM 基于 NousResearch/Nous-Hermes-2-Yi-34B 视觉侧基于 openai/clip-vit-large-patch14-336 输入模版及 tokenizer LLaVA 使用 LLM 的 Instruction 模版为基础, 使用 \u0026lt;image\u0026gt; 作为图像的 token, 作为 user message 的一部分, 整合到模版中. 以 liuhaotian/llava-v1.6-mistral-7b 为例, Mistral 系列的 Instruction 如下:\n1 \u0026lt;s\u0026gt; [INST] Instruction [/INST] Model answer\u0026lt;/s\u0026gt; [INST] Follow-up instruction [/INST] 引入图像后, LLaVA 使用如下的代码拼接图像和文本:\n1 2 3 4 5 # DEFAULT_IMAGE_TOKEN = \u0026lt;image\u0026gt; if model.config.mm_use_im_start_end: inp = DEFAULT_IM_START_TOKEN + DEFAULT_IMAGE_TOKEN + DEFAULT_IM_END_TOKEN + \u0026#39;\\n\u0026#39; + inp else: inp = DEFAULT_IMAGE_TOKEN + \u0026#39;\\n\u0026#39; + inp 最终得到的输入 prompt 如下:\n1 [INST] \u0026lt;image\u0026gt;\\nIs there any person in this photo [/INST] 对于多轮对话, 图像只在 prompt 中出现一次. 也就是说只有第一轮拼接了图像的 token, 之后的轮次, 按照模版扩展. 一个两轮对话的例子:\n1 2 [INST] \u0026lt;image\u0026gt; Is there any person in this photo [/INST] \u0026lt;s\u0026gt; No, there is no person visible in the photo. The image shows a wooden dock extending into a calm body of water, with a mountainous landscape in the background. The focus is on the dock and the serene natural setting. \u0026lt;/s\u0026gt; \u0026lt;/s\u0026gt;[INST] Give me all texts in this photo [/INST] 总之, LLaVA 的 tokenizer 通过增加 \u0026lt;image\u0026gt; 特殊字符作为图片的占位符, 有几张图片, 就要在 prompt 中有相应数量的 \u0026lt;image\u0026gt; 占位符.\n模型结构 目前 LLaVA 有 v1, v1.5, v1.6 几个版本, 不同版本之间的结构相同, 区别在于使用的 LLM 模型, 以及训练方法的不同, 模型列表可以参考 Model Zoo. 这里一并介绍其模型结构.\nLLaVA 主要有三部分组成: Pre-trained LLM, Pre-trained Vision Encoder, Projection Layers.\nPre-trained Vision Encoder 和 Projection Layers: 使用预训练的 CLIP-ViT-L/14 作为 vision encoder. 将输入的 image 表征为 $Z_{v}$, 再经过一个线性层 Projection Layers, 将 vision encoder 的编码 $Z_{v}$ 的维度转化为与 LLM embedding 空间相同的维度的表征 $H_{v}$, 从而可以作为 LLM 的输入.\nPre-trained LLM 选用的模型为 LLaMA2, Mistral, Vicuna, Yi 等系列的模型.\nForward 的过程是将文本 instruction prompt 通过 LLM 的 embedding 层, 得到文本 token 的表征 $H_{q}$, 然后将文本的表征序列与和图像的表征序列, 在序列维度上按特定的模版 concatnate 在一起, 得到新的更长的序列, 将这个序列输入到 LLM 进行生成.\n模型输入 这里指的是输入到 LLM 的 embeddings 矩阵是怎么构成的. 关键是序列中 \u0026lt;image\u0026gt; 图片占位符, 要怎么拼接到 embeddings 中.\n先说结果, 一张图片会先被切分为多个 patches, 然后和 resize 到相同维度的原始图片 concat 在一起, 传入到 Vision Encoder 得到每个像素的表征, 并进行 flatten, 将 patches, weight, height 都碾平到序列的样式, 形成 (像素总数量, 表征维度) 这样的尺寸, 然后作为图片的表征序列, 融合到文本序列中.\n比如 [INST] \u0026lt;image\u0026gt;\\nIs there any person in this photo [/INST] 这个 prompt, \u0026lt;image\u0026gt; 前面部分 [INST] 有 5 个 tokens, 后面部分 \\nIs there any person in this photo [/INST] 有 25 个 tokens, 被转换后的图像有 2700 个像素, 最终按 5 + 2700 + 25 的顺序组成一个长度为 2730 的 input_embed, 作为模型的输入.\n这是其中一张图片的处理方法, 如果输入有多张图片, 将每张图片的像素表征序列插入到对应的 \u0026lt;image\u0026gt; 图片占位符位置, 形成最终的长序列.\n可以看到, 图像占据了输入的绝大部分位置, 这是由于按像素表征, 还划分了 patches, 使得像素量远远大于文本 tokens 的数量.\n相关代码.\n处理图像\n根据原始输入图像的分辨率, 将图像 resize 到最合适的尺寸, 然后按固定的大小切分为若干个 patches. 将原始图片也 resize 到 patch 的尺寸, 然后将原始图片和切分的 patches 合并在一起, 并且原始图片放在第一位.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 def process_images(images, image_processor, model_cfg): image_aspect_ratio = getattr(model_cfg, \u0026#34;image_aspect_ratio\u0026#34;, None) new_images = [] if image_aspect_ratio == \u0026#39;pad\u0026#39;: for image in images: image = expand2square(image, tuple(int(x*255) for x in image_processor.image_mean)) image = image_processor.preprocess(image, return_tensors=\u0026#39;pt\u0026#39;)[\u0026#39;pixel_values\u0026#39;][0] new_images.append(image) elif image_aspect_ratio == \u0026#34;anyres\u0026#34;: for image in images: image = process_anyres_image(image, image_processor, model_cfg.image_grid_pinpoints) # 走这里 new_images.append(image) else: return image_processor(images, return_tensors=\u0026#39;pt\u0026#39;)[\u0026#39;pixel_values\u0026#39;] if all(x.shape == new_images[0].shape for x in new_images): new_images = torch.stack(new_images, dim=0) return new_images def process_anyres_image(image, processor, grid_pinpoints): \u0026#34;\u0026#34;\u0026#34; Process an image with variable resolutions. Args: image (PIL.Image.Image): The input image to be processed. processor: The image processor object. grid_pinpoints (str): A string representation of a list of possible resolutions. Returns: torch.Tensor: A tensor containing the processed image patches. \u0026#34;\u0026#34;\u0026#34; if type(grid_pinpoints) is list: possible_resolutions = grid_pinpoints else: possible_resolutions = ast.literal_eval(grid_pinpoints) best_resolution = select_best_resolution(image.size, possible_resolutions) # 选择最优的 resize 分辨率 image_padded = resize_and_pad_image(image, best_resolution) # resize 操作, 并进行 pad 操作 patches = divide_to_patches(image_padded, processor.crop_size[\u0026#39;height\u0026#39;]) # 将图像按固定的数值, 切分为多个 patches image_original_resize = image.resize((processor.size[\u0026#39;shortest_edge\u0026#39;], processor.size[\u0026#39;shortest_edge\u0026#39;])) # 将原始图片也 resize 到 patch 的尺寸 image_patches = [image_original_resize] + patches image_patches = [processor.preprocess(image_patch, return_tensors=\u0026#39;pt\u0026#39;)[\u0026#39;pixel_values\u0026#39;][0] for image_patch in image_patches] # 将原始图片和切分的 patches 合并在一起, 并且原始图片放在第一位 return torch.stack(image_patches, dim=0) 进行 tokenize\n将带有图片占位符的文本序列进行 tokenize, 得到 input_ids. 这里图片占位符只占一个 token, 对应的 token id 为 IMAGE_TOKEN_INDEX(-200).\n注意这里的 input_ids 不会直接丢给模型的 forward(训练) 或 generate(推理), 作用是引导图文的 embedding 融合得到 input_embed, 作为模型的输入.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 def tokenizer_image_token(prompt, tokenizer, image_token_index=IMAGE_TOKEN_INDEX, return_tensors=None): prompt_chunks = [tokenizer(chunk).input_ids for chunk in prompt.split(\u0026#39;\u0026lt;image\u0026gt;\u0026#39;)] def insert_separator(X, sep): return [ele for sublist in zip(X, [sep]*len(X)) for ele in sublist][:-1] input_ids = [] offset = 0 if len(prompt_chunks) \u0026gt; 0 and len(prompt_chunks[0]) \u0026gt; 0 and prompt_chunks[0][0] == tokenizer.bos_token_id: offset = 1 input_ids.append(prompt_chunks[0][0]) for x in insert_separator(prompt_chunks, [image_token_index] * (offset + 1)): input_ids.extend(x[offset:]) if return_tensors is not None: if return_tensors == \u0026#39;pt\u0026#39;: return torch.tensor(input_ids, dtype=torch.long) raise ValueError(f\u0026#39;Unsupported tensor type: {return_tensors}\u0026#39;) return input_ids 根据 tokenizer 得到的 input_ids 转化为 input_embed\n以 forward() 函数为例.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 def forward( self, input_ids: torch.LongTensor = None, attention_mask: Optional[torch.Tensor] = None, position_ids: Optional[torch.LongTensor] = None, past_key_values: Optional[List[torch.FloatTensor]] = None, inputs_embeds: Optional[torch.FloatTensor] = None, labels: Optional[torch.LongTensor] = None, use_cache: Optional[bool] = None, output_attentions: Optional[bool] = None, output_hidden_states: Optional[bool] = None, images: Optional[torch.FloatTensor] = None, image_sizes: Optional[List[List[int]]] = None, return_dict: Optional[bool] = None, ) -\u0026gt; Union[Tuple, CausalLMOutputWithPast]: if inputs_embeds is None: ( input_ids, position_ids, attention_mask, past_key_values, inputs_embeds, labels ) = self.prepare_inputs_labels_for_multimodal( input_ids, position_ids, attention_mask, past_key_values, labels, images, image_sizes ) return super().forward( input_ids=input_ids, # 这里的 input_ids 为 None attention_mask=attention_mask, position_ids=position_ids, past_key_values=past_key_values, inputs_embeds=inputs_embeds, # 融合了文本图像表征, 组成的序列表征 labels=labels, use_cache=use_cache, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict ) 合并的逻辑很复杂, 如下.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 def prepare_inputs_labels_for_multimodal( self, input_ids, position_ids, attention_mask, past_key_values, labels, images, image_sizes=None ): vision_tower = self.get_vision_tower() if vision_tower is None or images is None or input_ids.shape[1] == 1: return input_ids, position_ids, attention_mask, past_key_values, None, labels if type(images) is list or images.ndim == 5: if type(images) is list: images = [x.unsqueeze(0) if x.ndim == 3 else x for x in images] concat_images = torch.cat([image for image in images], dim=0) image_features = self.encode_images(concat_images) split_sizes = [image.shape[0] for image in images] image_features = torch.split(image_features, split_sizes, dim=0) mm_patch_merge_type = getattr(self.config, \u0026#39;mm_patch_merge_type\u0026#39;, \u0026#39;flat\u0026#39;) image_aspect_ratio = getattr(self.config, \u0026#39;image_aspect_ratio\u0026#39;, \u0026#39;square\u0026#39;) if mm_patch_merge_type == \u0026#39;flat\u0026#39;: image_features = [x.flatten(0, 1) for x in image_features] elif mm_patch_merge_type.startswith(\u0026#39;spatial\u0026#39;): new_image_features = [] for image_idx, image_feature in enumerate(image_features): if image_feature.shape[0] \u0026gt; 1: base_image_feature = image_feature[0] image_feature = image_feature[1:] height = width = self.get_vision_tower().num_patches_per_side assert height * width == base_image_feature.shape[0] if image_aspect_ratio == \u0026#39;anyres\u0026#39;: num_patch_width, num_patch_height = get_anyres_image_grid_shape(image_sizes[image_idx], self.config.image_grid_pinpoints, self.get_vision_tower().config.image_size) image_feature = image_feature.view(num_patch_height, num_patch_width, height, width, -1) else: raise NotImplementedError if \u0026#39;unpad\u0026#39; in mm_patch_merge_type: image_feature = image_feature.permute(4, 0, 2, 1, 3).contiguous() image_feature = image_feature.flatten(1, 2).flatten(2, 3) image_feature = unpad_image(image_feature, image_sizes[image_idx]) image_feature = torch.cat(( image_feature, self.model.image_newline[:, None, None].expand(*image_feature.shape[:-1], 1).to(image_feature.device) ), dim=-1) image_feature = image_feature.flatten(1, 2).transpose(0, 1) else: image_feature = image_feature.permute(0, 2, 1, 3, 4).contiguous() image_feature = image_feature.flatten(0, 3) image_feature = torch.cat((base_image_feature, image_feature), dim=0) else: image_feature = image_feature[0] if \u0026#39;unpad\u0026#39; in mm_patch_merge_type: image_feature = torch.cat(( image_feature, self.model.image_newline[None].to(image_feature.device) ), dim=0) new_image_features.append(image_feature) image_features = new_image_features else: raise ValueError(f\u0026#34;Unexpected mm_patch_merge_type: {self.config.mm_patch_merge_type}\u0026#34;) else: image_features = self.encode_images(images) # TODO: image start / end is not implemented here to support pretraining. if getattr(self.config, \u0026#39;tune_mm_mlp_adapter\u0026#39;, False) and getattr(self.config, \u0026#39;mm_use_im_start_end\u0026#39;, False): raise NotImplementedError # Let\u0026#39;s just add dummy tensors if they do not exist, # it is a headache to deal with None all the time. # But it is not ideal, and if you have a better idea, # please open an issue / submit a PR, thanks. _labels = labels _position_ids = position_ids _attention_mask = attention_mask if attention_mask is None: attention_mask = torch.ones_like(input_ids, dtype=torch.bool) else: attention_mask = attention_mask.bool() if position_ids is None: position_ids = torch.arange(0, input_ids.shape[1], dtype=torch.long, device=input_ids.device) if labels is None: labels = torch.full_like(input_ids, IGNORE_INDEX) # remove the padding using attention_mask -- FIXME _input_ids = input_ids input_ids = [cur_input_ids[cur_attention_mask] for cur_input_ids, cur_attention_mask in zip(input_ids, attention_mask)] labels = [cur_labels[cur_attention_mask] for cur_labels, cur_attention_mask in zip(labels, attention_mask)] new_input_embeds = [] new_labels = [] cur_image_idx = 0 for batch_idx, cur_input_ids in enumerate(input_ids): num_images = (cur_input_ids == IMAGE_TOKEN_INDEX).sum() if num_images == 0: cur_image_features = image_features[cur_image_idx] cur_input_embeds_1 = self.get_model().embed_tokens(cur_input_ids) cur_input_embeds = torch.cat([cur_input_embeds_1, cur_image_features[0:0]], dim=0) new_input_embeds.append(cur_input_embeds) new_labels.append(labels[batch_idx]) cur_image_idx += 1 continue image_token_indices = [-1] + torch.where(cur_input_ids == IMAGE_TOKEN_INDEX)[0].tolist() + [cur_input_ids.shape[0]] cur_input_ids_noim = [] cur_labels = labels[batch_idx] cur_labels_noim = [] for i in range(len(image_token_indices) - 1): cur_input_ids_noim.append(cur_input_ids[image_token_indices[i]+1:image_token_indices[i+1]]) cur_labels_noim.append(cur_labels[image_token_indices[i]+1:image_token_indices[i+1]]) split_sizes = [x.shape[0] for x in cur_labels_noim] cur_input_embeds = self.get_model().embed_tokens(torch.cat(cur_input_ids_noim)) cur_input_embeds_no_im = torch.split(cur_input_embeds, split_sizes, dim=0) cur_new_input_embeds = [] cur_new_labels = [] for i in range(num_images + 1): cur_new_input_embeds.append(cur_input_embeds_no_im[i]) cur_new_labels.append(cur_labels_noim[i]) if i \u0026lt; num_images: cur_image_features = image_features[cur_image_idx] cur_image_idx += 1 cur_new_input_embeds.append(cur_image_features) cur_new_labels.append(torch.full((cur_image_features.shape[0],), IGNORE_INDEX, device=cur_labels.device, dtype=cur_labels.dtype)) cur_new_input_embeds = [x.to(self.device) for x in cur_new_input_embeds] cur_new_input_embeds = torch.cat(cur_new_input_embeds) cur_new_labels = torch.cat(cur_new_labels) new_input_embeds.append(cur_new_input_embeds) new_labels.append(cur_new_labels) # Truncate sequences to max length as image embeddings can make the sequence longer tokenizer_model_max_length = getattr(self.config, \u0026#39;tokenizer_model_max_length\u0026#39;, None) if tokenizer_model_max_length is not None: new_input_embeds = [x[:tokenizer_model_max_length] for x in new_input_embeds] new_labels = [x[:tokenizer_model_max_length] for x in new_labels] # Combine them max_len = max(x.shape[0] for x in new_input_embeds) batch_size = len(new_input_embeds) new_input_embeds_padded = [] new_labels_padded = torch.full((batch_size, max_len), IGNORE_INDEX, dtype=new_labels[0].dtype, device=new_labels[0].device) attention_mask = torch.zeros((batch_size, max_len), dtype=attention_mask.dtype, device=attention_mask.device) position_ids = torch.zeros((batch_size, max_len), dtype=position_ids.dtype, device=position_ids.device) for i, (cur_new_embed, cur_new_labels) in enumerate(zip(new_input_embeds, new_labels)): cur_len = cur_new_embed.shape[0] if getattr(self.config, \u0026#39;tokenizer_padding_side\u0026#39;, \u0026#39;right\u0026#39;) == \u0026#34;left\u0026#34;: new_input_embeds_padded.append(torch.cat(( torch.zeros((max_len - cur_len, cur_new_embed.shape[1]), dtype=cur_new_embed.dtype, device=cur_new_embed.device), cur_new_embed ), dim=0)) if cur_len \u0026gt; 0: new_labels_padded[i, -cur_len:] = cur_new_labels attention_mask[i, -cur_len:] = True position_ids[i, -cur_len:] = torch.arange(0, cur_len, dtype=position_ids.dtype, device=position_ids.device) else: new_input_embeds_padded.append(torch.cat(( cur_new_embed, torch.zeros((max_len - cur_len, cur_new_embed.shape[1]), dtype=cur_new_embed.dtype, device=cur_new_embed.device) ), dim=0)) if cur_len \u0026gt; 0: new_labels_padded[i, :cur_len] = cur_new_labels attention_mask[i, :cur_len] = True position_ids[i, :cur_len] = torch.arange(0, cur_len, dtype=position_ids.dtype, device=position_ids.device) new_input_embeds = torch.stack(new_input_embeds_padded, dim=0) if _labels is None: new_labels = None else: new_labels = new_labels_padded if _attention_mask is None: attention_mask = None else: attention_mask = attention_mask.to(dtype=_attention_mask.dtype) if _position_ids is None: position_ids = None return None, position_ids, attention_mask, past_key_values, new_input_embeds, new_labels 训练过程 LLaVA 的训练包含两个阶段, 预训练和微调.\n预训练 预训练阶段是在 Image-Text Pair 数据集上进行的, 形式上是单轮训练. 从CC3M数据中过滤了595K Image-Text Pairs, 训练过程中, 只训练 Projection layers 中的参数, LLM 和 vision encoder 是冻结住的. 具体来说, CC3M 只包含了图像文本对, 可以看做是 (image, caption), 论文中使用 GPT-4 生成了一些多样化的 Instruction, 将简单的图像文本对扩展成了 (image, instruction, caption) 这种形式的三元组. 下图是生成的一些 instructions.\n预训练这个阶段是为了训练一个较好的 projection layer 可以将 visual feature 映射到 linguistic space, 也就是为了让 vision encoder 和 LLM 实现对齐.\n这个阶段结束之后模型获得了一个初步的理解图像的能力.\n微调阶段 微调阶段只冻结 vision encoder 中的参数, LLM 和 Projection layers 会放开训练. 这一阶段的目的是为了让模型更好地遵循用户给出的 Instruction.\n这个阶段分为两种任务: 多轮形式的 Instruct Tuning 和单轮形式的 Science QA 上的问答.\n微调阶段的模版如下, 这是多轮形式的, 如果样本是单轮任务, 只需要有一组 user-assistant pair. 其中计算损失的 token 是下图中的绿色序列和 tokens, 也就是说训练模型来预测模型的回答以及在哪里停止输出.\n以符号表示, 多轮形式的样本可以表示为, 其中 $T$ 代表了对话的轮次, $q$ 代表 user 轮次的文本输入, $a$ 代表模型回答轮次:\n$$ \\left(\\mathbf{X}_{\\mathrm{q}}^1, \\mathbf{X}_{\\mathrm{a}}^1, \\cdots, \\mathbf{X}_{\\mathrm{q}}^T, \\mathbf{X}_{\\mathrm{a}}^T\\right) $$\n上面 $X_{q}^{t}$ 代表的文本部分, 图片部分 $X_v$ 只在第一轮融入, 将文本和图像随机选择一种前后关系拼接在一起, 作为完整的 instruct.\n微调阶段的数据也是由 GPT-4 生成的. 由于 GPT-4 是纯文本输入, 所以要使用 GPT-4 来生成一些针对图片的问题和答案就需要将图片表示成GPT-4可以理解的形式.\n对于一张图片, 论文中用 5 句 caption 以及图片中 object 的 bounding box 的坐标数值来表示一张图片, 然后通过设计特定的 prompt 以及一些 examples, 让 GPT-4 生成针对一张图片的 conversation, detailed description, complex reasoning 三种类型的数据.\n参考资料 LLaVA Github LLaVA: Large Language-and-Vision Assistant 【vlm多模态大模型】llava解析 ","date":"2024-05-17T09:41:52+08:00","permalink":"http://localhost:1313/p/llava-insight/","title":"Llava Insight"},{"content":"模型介绍 LWM(Large World Model) 是一个多模态模型, 并且同时支持 1M 的上下文长度. 这里了解这样的模型是怎么训练出来的.\nWorld Model on Million-Length Video And Language With Blockwise RingAttention Huggingface Github 训练方法 Stage 1: 训练长上下文语言模型 第一阶段是训练纯文本模型 LWM-Text 和 LWM-Text-Chat. 上下文长度的扩展是渐进的, 从模型的原生长度到最终的 1M 长度, 中间会训练多个版本不同长度的模型.\n训练超长的上下文长度要占用大量的内存, 这里使用两个关键技术, 大幅降低训练长上下文的内存限制:\nRingAttention Blockwise Transformer 如何扩增模型的上下文长度 1. 模型结构支持 Ring attention with blockwise transformers for near-infinite context\nBlockwise parallel transformer for large context models\n由于传统 attention 结构在计算 attention weights 的平方复杂度, 而且现有的各种并行方案(DP, PP, TP)都需要将完整的序列投放到一个节点上, 因此单个节点的内存会限制训练样本的最长长度.\n这里需要使用 Blockwise RingAttention, 在序列维度上并行计算, 突破单个节点的内存限制, 这样能处理的长度只受节点数量的限制.\n论文中还进行了进一步的效率优化: 将 Blockwise RingAttention 与 FlashAttention 融合, 再结合 Pallas 进一步提升.\n2. 逐步训练 Growlength: Accelerating llms pretraining by progressively growing training length\n上一步通过 Blockwise RingAttention 突破了单点内存的限制, 但 attention 的平方级别的计算复杂度让计算仍然非常耗时.\n为了解决这个问题, 在训练过程中, 逐渐增加序列的长度, 从 32K 逐步增加到 1M tokens 的长度. 直觉上, 先打好 tokens 在 shorter-range 上依赖关系的基础, 然后再扩展到更长的序列上.\n由于每个样本的训练时间, 与样本长度成正比, 采用了上面的方案后, 相比与在最长(1M)序列长度上直接训练, 在相同的时间内, 训练的 tokens 总量明显扩大了数量级.\n上下文长度扩展的节奏如下:\nStep Context Doc Length Total Examples Total Tokens 1 32k 10k - 100k 78k 7B 2 128k 100k - 200k 92k 12B 3 256k 200k - 500k 37k 10B 4 512k 500k - 1M 3.5k 3B 5 1M 1M+ 0.8k 1B 3. RoPE 位置外推 为了扩展 position embedding 能够在长上下文中有更好的表现, 采用了一种简单的方法, 将 RoPE 中的参数 $\\theta$ 根据上下文的长度倍增. 原始版本的 $\\theta=10000$. 在这里长度与 $\\theta$ 的对应关系为:\n至于为什么简单地增加 $\\theta$ 就能够让 RoPE 在长上下文上有好的表现, 先看下面这张图. 这张图的值是 query 和 key 向量之间的 attention scores 期望在不同相对距离上的表现, 蓝色代表 $\\theta=10000$, 橙色代表 $\\theta=1000000$. 可以看到更大的 $\\theta$ 可以防止 attention score 在长距离上的衰减, 从而使得 far-away tokens 也能够对当前的预测产生贡献.\n在预训练阶段引入这种方法, 可以让 loss curves 更稳定, 特别是在低学习率上. 更具体的可以参考 Code llama: Open foundation models for code 这篇论文.\n如何准备训练数据集 预训练阶段使用数据集来自 The Pile Books3 dataset. 由于每个样本是一本书, 所以数据集中有超长的样本. 每个阶段使用的样本长度不同, 因此需要过滤出相应长度的样本.\n训练过程 从 LLaMA-2 7B 开始, 下表详细记录了每个阶段训练的详情. 一个阶段训练结束后, 作为下一个阶段的初始化.\n长上下文 SFT 用 Book3 数据集完成预训练后, 还需要进行 Chat Fine-tuning 以让模型掌握指令跟随的能力 / 聊天能力.\nSFT 数据集准备(重点) 将 Book3 数据集中的样本进行分块(chunk), 每块大小为 1000 个 tokens. 将每个 chunk 通过 prompt 编排后输入到短上下文的 LLM 中生成一个 Question-Answer 对. 得到一批这样的 chunk 和 QA 对组合.\n当我们需要对长上下文的预训练模型进行 SFT 时, 例如对 32K 上下文长度的模型, 我们要拼接出一个包含 32K tokens 的样本, 方法将相邻的 chunk 拼接在一起, 并且将这些 chunks 对应的 QA 组织成 Chat 的形式, 拼接在这个样本的最后.\n最后采用的数据集来自两部分, 一部分是 UltraChat 数据集, 另一部分是用上面的方法生成的 QA 数据集, 这两部分的比例为 7: 3. 对于 UltraChat 数据集, 也要提前 pack 为训练模型的序列上下文上限的长度, 这点非常重要.\n由于 UltraChat 多为短的 chat sequences, 因此 packed 后的样本, 需要计算 loss 的 tokens 的比例是大大超过我们合成的数据集的(要计算 loss 的 tokens 是对话中的 answer 部分, 合成数据集的样本中大部分都是 chunk, 这部分不计算 loss, 统计下来合成数据集的这个比例小于 1%). 所以 UltraChat 和合成数据集中的样本, 一定不要混合在一起进行 packing, 而是要分开 packing.\n我们在 4 个长度上进行了 SFT 训练, 训练拿对应长度的预训练模型进行初始化.\nStage 2: 训练长上下文的多模态模型 经过 Stage 1 得到训练好的 LWM-Text 和 LWM-Text-Chat, 在 Stage 2 的目标是在 long video and language 序列上完成高效的联合训练.\n如何修改模型架构以融合视觉 模型的整体结构如下图所示. LWM 是一个支持 1M tokens 序列的自回归 transformer. 每个视频帧被 tokenize 成 256 个 tokens. 这些视频帧 tokens 与 text tokens 拼接后, 送入到 transformer 中预测下一个 token, 这个 token 可能是 text token 也可能是 vision token.\n视觉编码器使用的是 VQGAN, 将 $256 \\times 256$ 的图片输入 tokenize 成 $16 \\times 16$ 的离散 tokens. 对于视频, 使用 VQGAN per-frame 对视频进行 tokenizing.\n为了在生成过程中区分两种模态, 知道何时进行切换, 需要标记\ntext generation 的结束和 vision generation 的开始 vision generation 的结束和 text generation 的开始 为此引入, 为了定义视觉生成的结束, 引入了两个新的 mark token:\n\u0026lt;eof\u0026gt;, end of frame. 在每个视频帧(除去整个视频的最后一帧)生成后添加 \u0026lt;eov\u0026gt;, end of video. 在生成的视频的最后一帧后添加, 以及如果生成的是单张图片, 在生成的图片后也引入这个符号 另外, 为了定义 text generation 的结束, 使用 \u0026lt;vision\u0026gt; 和 \u0026lt;/vision\u0026gt; 将 vision tokens 包围住.\n需要注意的是 \u0026lt;eof\u0026gt; 和 \u0026lt;eov\u0026gt; 各自对应一个特殊 token, 而 \u0026lt;vision\u0026gt; 和 \u0026lt;/vision\u0026gt; 不是特殊 token, 要作为 text 对待, 使用 tokenizer 转化为对应的 tokens.\n输入输出中不同类别的 tokens 在训练集中有不同的拼接顺序, 包含:\nimage-text text-image video, 也就是多个 images text-video text 上面模型的架构图中就是一种 image-text 的拼接形式.\n训练过程 使用预训练得到的 LWM-Text-1M 语言模型进行初始化. 而且跟上面训练纯文本的模型一样, 也是分多步, 逐渐扩大多模态模型的上下文长度, 最终得到一个 1M 上下文大小的多模态模型.\n这个多步逐渐扩大长度训练的过程, 使用的数据是 text-image 和 text-video 数据的混合. 另外与训练纯文本不同的是, 由于我们用 LWM-Text-1M 进行初始化, 模型已经支持了 1M 上下文的长度, 因此在这里训练多模态能力时, RoPE 的 $\\theta$ 就不再使用纯文本中的倍数扩增, 而是使用固定值 $\\theta=50\\text{M}$. 一个阶段训练结束后, 作为下一个阶段的初始化. 各个阶段训练的情况如下:\n每个阶段使用的训练集如下.\nLWM-1K: 使用的是 text-image dataset, 由 LAION-2B-en 和 COYO-700M 两个数据集混合得到. 过滤掉分辨率不足 256 的样本, 总共收集了大约 1B 个 text-image 数据对 在训练过程中, 将 text-image pairs 拼接起来, 并且随机将两种模态的顺序进行交换, 来建模: text-image generation 任务 unconditional image generation 任务 image captioning 任务 pack text-image pairs 达到 1K 的 tokens 序列长度 LWM-8K: 使用的是 text-video 训练集, 由 WebVid10M 和 3M InternVid10M 混合得到. 把 images 和 video 看成两种模态的话, 这里的数据集这两种模态各占 50%. 将 30 帧的视频帧转换为 4FPS 将 images pack 成 8K 的序列长度 同样的, 随机对每个 text-video pair 中两种模态的顺序进行交换 LWM-Chat-32K/128K/1M: 最后 3 个阶段, 混合了以下四种下游任务分别对应的 chat data: text-image generation image understanding text-video generation video understanding 其中 text-image generation 和 text-video generation 是从多模态预训练数据中抽取了子集, 并按 chat format 构造了数据集 image understanding 使用了 ShareGPT4V 中的 image chat instruct data video understanding 使用了 Valley-Instruct-73K 和 Video-ChatGPT-100K 两个数据集混合后其中的 instruct data 对于 text-image generation, image understanding, text-video generation 这三类 chat data, 属于 short context data, 使用 packing 方法将他们拼接成要训练的上下文长度 Packing 之后, 在计算 attention 的时候, 要特别注意 mask 的方案, 每个 text-vision pair 只能看到它们自己这对 对于 video understanding data, 如果视频太长, 会采样一个满足训练上下文长度的最大数量的帧数 在训练过程中, 对于每个 batch, 为 4 个任务各分配 25% 的比例 对于 LWM-1K 和 LWM-8K 这前两个阶段, 还增加混合了 16% 的 pure text data, 使用的是 OpenLLaMA 数据集, 以防止语言能力在多模态训练过程中退化. 混合的方式是一整个 batch 都是 pure text data, 相当于多了 16% 的 pure text batch.\n","date":"2024-05-16T23:39:39+08:00","permalink":"http://localhost:1313/p/lwm-insight/","title":"LWM Insight"},{"content":"\nPre-training 数据 对数据源进行了限制, 只从具有很高真实性的数据源中获取数据, 并进行 up-sampling, 增强知识, 抑制幻觉 数据规模 2T tokens.\n训练细节 模型结构细节 Pre-Norm with RMSNorm SwiGLU activation function RoPE GQA, grouped-query attention 训练参数 训练使用的 learning rate 和 context length 因模型大小而异, 详情见下图\n优化器 AdamW, $\\beta_{1}=0.9$, $\\beta_{2}=0.95$, $\\varepsilon=10^{-5}$, $\\text{weight decay}=0.1$ Warmup: 2000 steps Cosine learning rate schedule, 最终学习率衰减到最大学习率的 10% Gradient clipping: 0.1 Learning rate: 7B: $3 \\times 10^{-4}$ 13B: $3 \\times 10^{-4}$ 34B: $1.5 \\times 10^{-4}$ 70B: $1.5 \\times 10^{-4}$ 词表大小: 32k 最终损失降低到:\n7B: 1.75 13B: 1.77 34B: 1.57 70B: 1.50 SFT 数据收集 人工编写 Prompt + Answer, 收集了 27540 高质量的 SFT data. 高质量数据包括两大类:\nhelpfulness. 样本的 response 确实可以解决 prompt 的任务 safety. 对于不安全的 prompt 拒绝回答 Meta 在 SFT 这一步, 只收集了 20k+ 量级的. 做出这个决策的原因是, 在使用这个量级的数据 SFT 之后, 模型的输出, 与人类标注的质量可以相比较. 因此团队认为 SFT 的标注工作可以结束, 将标注资源放在 RLHF 要使用的偏好数据的标注.\n训练参数 Batch Size: 64 Learning Rate: 2e-5 Learning Rate Schedule: cosine learning rate schedule Sequence Length: 4096 Weight decay: 0.1 Epochs: 2 使用了 Packing 策略, 将训练集中所有的 prompts 和 answers 连接在一起后按长度切分, 保证序列长度被完全使用. 使用一个特殊符号作为 prompt 和 answer 的分隔.\nRLHF RLHF 的目标是将模型的输出行为对齐于人类偏好(human preferences)和遵循指令(instruction following).\n收集人类偏好数据 收集人类偏好数据(human preference data)来训练奖励模型. 收集的方法如下:\n人工编写 prompt 将编写的 prompt 输入到 SFT 后的模型中, 得到两个输出作为采样, 并为这两个采样标注哪个回答更好 为了让采样更具有多样性, 使用同一个 prompt 采样时, 使用不同的模型(model variants)进行采样(猜测是训练了两个 SFT 模型), 并且使用了不同的 temperature. 为偏好划分了 4 种标签: significantly better, better, slightly better, negligibly better/ unsure 偏好标注的关注点, 在与回答的 有用性(helpfulness) 和 安全性(safety) 两个方面, 因此判断四种标签的方法为:\nhelpfulness: LLaMA2-Chat 的回答可以满足用户要求, 提供所需的信息 safety: 模型的回答是否是安全的, 标签被设计为 3 类: 选择的回答更安全, 另外的回答不安全. 最终占整个数据集的 18% 两个回答都是安全的. 47% 两个回答都是不安全的. 35% 两者的标注是分开的. 例如 giving detailed instructions on making a bomb 的回答可以被认为有用, 但是不安全的. 这种分开标注, 相互不纠缠, 有更清晰的标注引导, 标注的质量会更高.\n从 safety 的三类标签也能看到, 抛弃了 选择的回答是不安全, 另外的回答是安全的 这种情况, 因为安全的回答才有资格作为更好的答案.\n最终收集了 1,418,091 条人工偏好数据.\nSafety 偏好数据 safety 方面, 针对性地编写了一些 对抗性的 prompt(adversarial prompts), 从两个角度进行了编写.\nRisk category Risk category, 可以理解为是可能产生不安全内容的潜在主题(topic). LLaMA 2 中划分了三个类别:\nillicit and criminal activities: 各种犯罪行为 hateful and harmful activities: 歧视, 诽谤, 自残等行为 unqualified advice: 例如医疗建议, 金融建议, 法律建议等各种严肃建议的场景 Attack vectors Attack vectors 可以理解为 prompt 的多种提问风格, 这种风格可以诱发模型做出不好的回答. 考虑了以下几种:\npsychological manipulation: 心理操纵 logic manipulation: 逻辑操纵, 如虚假假设 syntactic manipulation: 句法操纵, 如故意的错误拼写, 汉语中还有形近字, 音近字, 拆字等攻击 semantic manipulation: 语义操控, 如隐喻, 阴阳怪气.. perspective manipulation: 透视操纵, 如不合适的角色扮演 收集-训练迭代 定期收集人工标注数据, LLaMA 2 中每周收集一次.\n在收集到更多的人工偏好数据后, 训练得到更好的奖励模型, 再通过 PPO 训练, 得到更好的 Chat 模型.\n在得到更好的 Chat 模型之后, 从 Chat 模型中采样得到的数据分布也会发生变化, 这会让模型产生一些新的数据分布. 这些新数据分布, 在下轮的训练中, 会拓宽奖励模型的视野, 提高模型的泛化性和整体性能.\n这种一轮轮迭代的方式, 帮助奖励模型的分布不断拓宽, 进而经过 RLHF 的 Chat 模型也进一步提升.\nReward Modeling 有研究发现 helpfulness 和 safety 存在 trade off 的情况, 这会使得用同一个奖励模型, 在这两个评价任务中都得到好的效果, 是非常有挑战性的. 因此, LLaMA 2 分开训练了两个奖励模型: Helpfulness RM 和 Safety RM.\n奖励模型使用预训练模型的 checkpoints 作为初始化, 保证模型具备预训练过程中获取的知识, 防止出现奖励模型和 RLHF 训练目标的 Chat 模型, 出现知识不匹配的情况, 进而导致产生幻觉.\n训练目标 使用 Binary ranking loss, 目标是让 RM 对偏好的回复产生更高的分数.\n$$ \\mathcal{L}_{\\text {ranking }}=-\\log \\left(\\sigma\\left(r_\\theta\\left(x, y_c\\right)-r_\\theta\\left(x, y_r\\right)-m(r)\\right)\\right) $$\n$r_\\theta\\left(x, y_c\\right)$ 代表 Prompt $x$ 和 Chat 模型的 completion $y$ 给到 RM 模型 $\\theta$ 得到的标量分数. $y_{c}$ 代表是标注人员更偏好的回复, $y_{r}$ 代表的是被拒绝的回复.\nMeta 进一步引入了 margin 成分 $m(r)$. Binary ranking loss 是减函数, 两个 completion 之间的 score 差距越大, 对应的损失就越小, 这也符合直觉. 而在融入减去一个非负的 margin 成分之后, 缩小了 completion 之间的 score 差别, 产生更大的损失, 迫使模型将两者之间的距离拉的更远.\n$m(r)$ 是一个离散函数, 它利用了偏好数据的 4 种标签: significantly better, better, slightly better, negligibly better/ unsure, 显式地利用这些标签来对不同差别的 completion 施加不同的 margin 大小, 整体上是对差异更显著的 completion 增加更大的 margin, 以期望拉开更大的差距.\nMeta 实验了两套大小不同的 $m(r)$ 离散函数 , 对应于下表中的 Margin Small 和 Margin Large, 实验证明增加的 margin 项确实提升 RM 的性能, 且更大的尺度的 $m(r)$ 对应的效果更好, 但更大尺度的 $m(r)$ 对相近回答的性能有所降低.\n$m(r)$ 离散函数详情:\n实验结果, 数值代表的是准确率:\n训练数据 训练 RM 的数据集, 由上面所收集的人工偏好数据和开源偏好数据集组合而成.\n初始时, 由于没有人工数据, 使用开源偏好数据训练得到初版 RM 模型, 同时并行地收集人工偏好数据. 这里有一个矛盾点, RLHF 需要的 reward single, 应当是对 LLaMA 2-Chat 模型的输出进行的学习, 而开源数据是其他模型产生的. 但在论文的实验中, 没有观察到开源数据带来任何的负迁移效果. 因此最终的数据集混合了这些开源数据, 而这些开源数据, 会为 RM 带来更好的泛化性能.\n对于生成数据和开源数据这两种数据集, Helpfulness RM 和 Safety RM, 使用了不同的最佳混合比例\nHelpfulness RM 的训练集 使用了全部的 人工标注得到的 Helpfulness 数据, 占数据集的一半 另外一半是从 人工标注得到的 Safety 数据 和 开源数据集 中采样得到 Safety RM 的训练集 使用了全部的 人工标注得到的 Safety 数据, 以及 Anthropic Harmless data, 这部分占整体的 90% 另外 10% 混合了 人工标注得到的 Helpfulness 数据 和 开源的 helpfulness 数据集. 混合了这 10% 的 helpfulness 数据集之后, 能够有效的提升 Safety RM 在 chosen 和 rejected 都是安全的样本中的准确率 训练参数 Epoch: 1, 训练的步数多会导致 over-fitting Learning Rate 5e-6 for 70B 1e-5 for other scales Learning Rate Schedule: cosine learning rate schedule, 最低降到 learning rate 的 10% Warm-up: 3% total steps Batch size: 512 pairs per batch Weight decay: 0.1 强化学习 训练策略 Meta 探索了两种 RLHF 中常用的两种微调算法:\nPPO(Proximal Policy Optimization) 拒绝采样微调(Rejection Sampling fine-tuning) 这两种 RL 算法的区别在于:\n拒绝采样对于每个 prompt, 采样出 K 个样本; PPO 只采样一个样本 PPO 在每一步 policy 模型参数更新后, 对当前训练的 prompt 进行采样; 拒绝采样时在强化学习开始之前, 从初始的 policy 中对所有的 prompt 都进行采样, 一次性采样得到所有输出, 这个其实就是 SFT Meta 在强化学习这一步, 使用迭代训练的策略. 由于 RM 使用的偏好训练集是一批批采样标注得到的, 使用新标注的数据得到更好的 RM 模型, 并获取更多的 prompts, 这是训练更好的 RLHF 模型的基础. 实际上, Meta 训练了 RLHF-V1 到 RLHF-V5 共 5 个版本的模型.\n在包括 RLHF-V4 在内的早期版本中, 只使用拒绝采样进行训练. 在 V5 中, 顺序的使用这两种方法, 先使用拒绝采样方法训练, 再挑选出 Evaluation 最高的 checkpoint, 使用 PPO 继续进行训练.\n拒绝采样 拒绝采样的做法 拒绝采样本质上就是在进行 SFT, 只是用来训练的样本是从模型中采样得到的.\n拒绝采样这种 RL 方法首先在 70B 规模的 LLaMA2-Chat 模型上进行, 采样得到的样本, 除了用来训练 70B 的模型, 所有更小规模的模型也是用这些数据做拒绝采样的训练, 而不是各个规模的模型各自自己采样. 这样做的目的, 是将大模型的能力蒸馏到小模型中去.\nRLHF 共经过了 RLHF-V1 到 RLHF-V5 5 个阶段, 每个阶段的训练中, 对于每个 prompt 样本, 使用上个阶段得到的样本采样 K 个 answers, 并选择当前最优的 RM 进行评价, 得到分数最高的样本. 在早期的探索中, 训练当前阶段使用的样本, 都是用上个阶段的模型采样得到, 例如训练 RLHF-V3 使用的样本全部来自 RLHF-V2 的采样. 但这种方法在整体指标提升的同时, 会导致某些方面能力的退化. 例如通过这种方法训练得到的 RLHF-V3 在编写押韵诗句方面比之前的版本更差.\n为了解决这个问题, 每个阶段的训练, 会使用之前所有阶段产生的样本作为候选池, 从中选出 score 最高的一批样本作为训练数据集. 例如, 训练 RLHF-V3 会使用 RLHF-V2 和 RLHF-V1 的样本.\n拒绝采样为什么能起作用 为什么这种 SFT 的形式可以用作 RLHF 呢?\n上图呈现了单个 prompt 随着采样数量 N 的增加, 这 N 个采样 reward score 的最大值和中位数的变化情况. 每个点的值是统计训练集中所有 prompt 值的平均得到. 拒绝采样用的是 N 个采样中 score 最大的样本作为训练样本, 对应着图中的蓝线. 而橙色线代表着一次采样的期望值, 因此最大值和中间值之间的 delta 就代表了拒绝采样使用最优采样做 fine-tuning 的潜在收益.\n随着单个 prompt 采样量 N 的增加, 这个潜在收益还会逐渐增大, 或者说探索的广度越大, 收益也会越大, 是一种用广度换深度(PPO每次采样后更新模型, 再采样循环的方法可以看做是在深度上进行探索)的策略.\n因此温度这个超参数对探索也会起到至关重要的作用, 更高的温度能够让采样结果的多样性增加, 但也不是用最大温度就可以.\n上图反应了 LLaMA2-Chat-SFT 和 LLaMA2-Chat-RLHF 两个模型采样 N 个样本对应的最大 reward score 的曲线, 每条曲线代表着一个采样温度. 可以观察到, 采样的最优温度, 在迭代训练的过程中是在变化的.\n因此在每个阶段的 RLHF 模型训练过程中, 每训练一定的步数, 就要暂停下来, 在不同的温度下采样一批, 根据 max reward score 评估当前阶段的最优采样温度.\n类似的做法 LLaMA2 中使用的拒绝采样方法, 与 RAFT: Reward rAnked FineTuning for Generative Foundation Model Alignment 中提到的方法完全相同.\n另外还有 RRHF: Rank Responses to Align Language Models with Human Feedback without tears 方法. 对某一个 prompt, 通过不同的方式(LLM 采样, 人类专家等)生成很多个 answers, 让 RM 去打分, 然后选出好的去 Fine-tune 大模型. 此外, 让大模型也算一下这些 answers 的似然概率当做评分, 让这个评分和RM的打分尽可能对齐, 这个对齐通过设置 Rank loss 进行监督学习来实现.\nPPO PPO 中的 reward function 如下. 这里由于有 Helpfulness RM 和 Safety RM 两种, 所以 reward function 中的 RM score 部分, 需要混合这两个模型的输出.\n$$ R(g \\mid p)=\\bar{R}_c(g \\mid p)-\\beta D_{K L}\\left(\\pi_\\theta(g \\mid p) | \\pi_0(g \\mid p)\\right) $$\n其中 $\\bar{R}_c(g \\mid p)$ 是混合后 RM 的输出, 它的定义如下:\n$$ \\begin{aligned} \u0026amp; R_c(g \\mid p)= \\begin{cases}R_s(g \\mid p) \u0026amp; \\text{if IS\\_SAFETY}(p) \\text { or } R_s(g \\mid p)\u0026lt;0.15 \\\\ R_h(g \\mid p) \u0026amp; \\text { otherwise }\\end{cases} \\end{aligned} $$\n$$ \\tilde{R}_c(g \\mid p)=\\operatorname{WHITEN}\\left(\\operatorname{LOGIT}\\left(R_c(g \\mid p)\\right)\\right) $$\n其中 $R_s(g \\mid p)$ 代表 Safety RM 的输出, $R_h(g \\mid p)$ 代表 Helpfulness RM 的输出. 优先考虑安全性. 其中人工编写的 prompt 已经标记了哪些是可能引发不安全回答的, 对应上式中的 $\\text{IS\\_SAFETY}(p)$, 对于这部分可能引发安全问题的样本, 以及 Safety RM 输出的分数小于 0.15 的阈值的不安全回答, 这两类的不安全情况, 优先考虑安全分数. 0.15 阈值对应的是 Safety RM 0.89 的准确率和在 Meta Safety test set 上 0.55 的召回率.\n其他安全的情况, 再考虑 Helpfulness RM 的输出.\n在混合了两种 RM 的输出后, $\\text{LOGIT}$ 是 sigmoid 的反函数, 再进行白化, 目的是增加稳定性, 与 KL 散度损失项取得合适的平衡.\n训练参数 使用 AdamW 优化器, 对应的参数为 $\\beta_{1}=0.9$, $\\beta_{2}=0.95$, $\\varepsilon=10^{-5}$, $\\text{weight decay}=0.1$ Gradient clipping 1.0 Learning rate: 1e-6 PPO Micro batch size 64, batch size 512, PPO clip threshold 0.2 损失函数中的 KL 系数, 在 7B 和 13B 模型中 $\\beta=0.01$, 在 34B 和 70B 模型中 $\\beta=0.005$ 不同规模的模型训练步数在 200 到 400 步之间, 并构建了 dev prompts 数据集, 做 early stopping Multi-Turn 一致性 / Ghost Attention 在对话过程中, 往往会有一个 System Message, 比如要求回复简洁, 或者要求 assistant 扮演一个角色. 期望是这个 System Message, 在整个对话过程中持续作用.\n但在最初的 RLHF 迭代中, 得到的模型在经过几轮的交谈后, 就倾向于遗忘最初的 system message 中的要求了.\n为了解决这个问题, 提出了 Ghost Attention(GAtt). 这不是一种模型结构, 而是一种调整 fine-tuning 数据的方式.\nGhost Attention\n把一个 $n$ 轮的多轮对话记为 $\\left[ u_1,a_1,\\cdots,u_n,a_n \\right]$, 把 system message 定义为 $\\text{inst}$. 接下来, 将 $\\text{inst}$ 与每轮 user message $u_i$ 拼接起来.\n然后使用最新的 RLHF 模型, 对这个多轮对话中每个拼接后的 user message 进行多次采样, 使用 RM 选择最好作为训练样本(拒绝采样的方法). 这样就得到了符合指令的多轮对话数据.\n再将除了第一轮之外的所有 user message 中的 $\\text{inst}$ 部分删除, 用这个数据进行 fine-tuning.\n在训练时, 为了实现多轮以后的 system 指令遵从能力, 将多轮数据中前面几轮中所有 tokens 的 loss 置为 0, 包括原本要计算 loss 的 assistant message 部分.\n以上就是 Ghost Attention 的做法.\n论文中 system message 从一些任务中合成得到:\nHobbies, 例如 You enjoy e.g. Tennis Language, 例如 Speak in e.g. French Public Figure, 例如 Act as e.g. Napoleon 而 Hobbies 和 Public Figure 合成用到的 list, 是通过 LLaMA2-Chat 生成, 以防止出现模型知识不具备的情况, 会加重幻觉. 而为了让 system instruction 更复杂以及更有多样性, 最终的 instruction 是随机地将上面的几类组合得到. 同时还会将 instruction 进行简化, 例如 Always act as Napoleon from now -\u0026gt; Figure: Napoleon. 使用这些方法, 生成了一个 SFT 数据集, 用这个数据集微调出 LLaMA2-Chat.\n总结 SFT的必要性和问题\n每个标注人员的创作的 prompt 和 completion 都有很大的多样性, 在这些数据集上进行 SFT, 得到的模型可以学习这种多样性.\n这种多样性是很长尾的, 这些长尾数据中也会有很多不好的结果, 这些是 SFT 无法解决的.\nRLHF的作用\n比较两个 completion 哪个更好, 这个任务相对更简单, 因此 RM 可以很快地学习到将低分分配给效果不好的长尾分布, 使得最差的答案在分布上逐渐被删除. 这一点是通过人类偏好数据标注和 RLHF 训练过程配合得到的.\nLLM 超越人类能力的上限, 是通过 RLHF 的人类监督信号得到的, 它比 SFT 更重要.\n","date":"2024-05-13T23:11:05+08:00","permalink":"http://localhost:1313/p/llama-2-insight/","title":"Llama 2 Insight"},{"content":"Paper: DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model\nGithub: DeepSeek-V2\n模型结构 模型整体结构: MLA + MoE\nMLA: 在大幅缩小 KV Cache 的同时, 获得超越 MHA 的效果 MoE: 共 236B 参数, 每个 token 激活 21B 参数 MLA MHA 在推理阶段的劣势 DeepSeek-V2 对 Transformer 架构中的自注意力机制进行了全方位的创新, 使用 MLA(Multi-head Latent Attention) 结构.\nTransformer 标准的 MHA(Multi-Head Attention) 结构中, $n_h$ 为 attention heads 数量, $d_h$ 为每个 head 内部的维度, $\\mathbf{h}_t \\in \\mathbb{R}^d$ 代表了当前 attention layer 层中第 $t$ 个 token 的输入. 标准的 MHA 通过三个不同的参数矩阵 $W_Q,W_K,W_V \\in \\mathbb{R}^{d_h n_h \\times d}$ 得到 $\\mathbf{q}_t, \\mathbf{k}_t, \\mathbf{v}_t \\in \\mathbb{R}^{d_h n_h}$, 对应于:\n$$ \\begin{aligned} \u0026amp; \\mathbf{q}_t = W_Q \\mathbf{h}_t \\\\ \u0026amp; \\mathbf{k}_t = W_K \\mathbf{h}_t \\\\ \u0026amp; \\mathbf{v}_t = W_V \\mathbf{h}_t \\end{aligned} $$\n在 MHA 中, $\\mathbf{q}_t, \\mathbf{k}_t, \\mathbf{v}_t$ 被拆分为 $n_h$ 个 heads, 然后每个 head 中进行 attention 计算:\n$$ \\begin{aligned} \u0026amp; {\\left[\\mathbf{q}_{t, 1} ; \\mathbf{q}_{t, 2} ; \\ldots ; \\mathbf{q}_{t, n_h}\\right]=\\mathbf{q}_t,} \\\\ \u0026amp; {\\left[\\mathbf{k}_{t, 1} ; \\mathbf{k}_{t, 2} ; \\ldots ; \\mathbf{k}_{t, n_h}\\right]=\\mathbf{k}_t,} \\\\ \u0026amp; {\\left[\\mathbf{v}_{t, 1} ; \\mathbf{v}_{t, 2} ; \\ldots ; \\mathbf{v}_{t, n_h}\\right]=\\mathbf{v}_t,} \\\\ \u0026amp; \\mathbf{o}_{t, i}=\\sum_{j=1}^t \\operatorname{Softmax}_j\\left(\\frac{\\mathbf{q}_{t, i}^T \\mathbf{k}_{j, i}}{\\sqrt{d_h}}\\right) \\mathbf{v}_{j, i}, \\\\ \u0026amp; \\mathbf{u}_t=W^O\\left[\\mathbf{o}_{t, 1} ; \\mathbf{o}_{t, 2} ; \\ldots ; \\mathbf{o}_{t, n_h}\\right], \\end{aligned} $$\n每个 head 计算得到的结果再拼接起来, 经过参数矩阵 $W_O \\in \\mathbb{R}^{d \\times d_h n_h}$ 得到 MHA 的输出张量.\n在推理阶段, 所有的 keys 和 values 作为 KV Cache, 需要被缓存, 以加速推理. 而根据上式, MHA 结构做 KV Cache, 每个 token 需要缓存 $2n_h d_h l$ 个参数, $l$ 代表 attention layer 数量. MHA 的 KV Cache 量是非常大的, 这会对最大化推理阶段可以支持的 batch size 和 sequence length 是瓶颈.\nLow-Rank Key-Value Joint Compression 低秩KV联合压缩(Low-Rank Key-Value Joint Compression)是 MLA 结构的核心, 可以大幅缩小 KV Cache 的占用.\n$$ \\begin{aligned} \\mathbf{c}_t^{K V} \u0026amp; =W^{D K V} \\mathbf{h}_t \\\\ \\mathbf{k}_t^C \u0026amp; =W^{U K} \\mathbf{c}_t^{K V} \\\\ \\mathbf{v}_t^C \u0026amp; =W^{U V} \\mathbf{c}_t^{K V} \\end{aligned} $$\n$\\mathbf{c}_t^{K V} \\in \\mathbb{R}^{d_c}$ 是压缩后的可训练隐向量, 目标是将 keys 和 values 中的信息压缩进去. $d_c(\\ll d_h n_h)$ 代表了隐向量的维度. $W^{D K V} \\in \\mathbb{R}^{d_c \\times d}$ 是输入的下投影矩阵(down-projection matrix), 将输入的维度由 $d$ 压缩到 $d_c$; $W^{U K}, W^{U V} \\in \\mathbb{R}^{d_h n_h \\times d_c}$ 是 keys 和 values 的上投影矩阵(up-projection matrices).\n推理过程中, MLA 只需要为每个 token 缓存 $\\mathbf{c}_t^{K V}$, 这样每个 token 的 KV Cache 大小减少为 $d_c l$ 个参数.\n下图对比了常见的 attention 方法 MHA, GQA, MQA 和 MLA 之间在 KV Cache 上的区别. 每个子图代表了某个 token 的计算情况, 所有子图中都有 8 个 heads. 上面这种 KV joint compression 的方法确实可以更进一步地节省 KV Cache 的占用, 虽然在计算量上有增加. 另外为了降低训练过程中 activation 的存储占用, 对 queries 也进行了 low-rank compression, 但这个操作不会降低 KV Cache 的占用, 是为了降低训练时的消耗.\n$$ \\begin{aligned} \\mathbf{c}_t^{Q} \u0026amp; =W^{DQ} \\mathbf{h}_t \\\\ \\mathbf{q}_t^C \u0026amp; =W^{U Q} \\mathbf{c}_t^{Q} \\end{aligned} $$\n$\\mathbf{c}_t^{Q} \\in \\mathbb{R}^{d_c^{\\prime}}$ 代表了压缩后的隐向量, $d_c^{\\prime}(\\ll d_h n_h)$ 代表了压缩后隐向量的维度, $W^{DQ} \\in \\mathbb{R}^{d_c^{\\prime} \\times d}$, $W^{UQ} \\in \\mathbb{R}^{d_h n_h \\times d_c^{\\prime}}$ 分别是下投影和上投影矩阵.\nRoPE with MLA Low-Rank compression 一个最大的问题是与 KV Cache 不兼容. RoPE 需要作用在 queris 和 keys 上, 而作为新的 KV Cache 的 $\\mathbf{c}_t^{K V}$ 在计算得到 $\\mathbf{k}_t^C$ 之后, 再施加 RoPE, 这样在生成每个 token 时, 都需要对这个 token 之前的所有 token, 都重新计算得到他们的 keys, 这就丧失了 KV Cache 存在的意义, 大幅降低了推理的效率.\n为了解决这个问题, 设计了两种不同作用的 queries 和 keys:\n其中一类 queries 和 keys 不去融合 RoPE, 这些可以完全发挥 Low-Rank compression 的作用 另外一类queries 和 keys 去融合 RoPE, 这一类就无法再去使用 Low-Rank compression, 而是采用了 MQA(Multi-Query Attention) 的方式, 所有的 query 共享同一个 keys 这两种 queries 和 keys 在计算 Multi-head attention 之前经过各种计算得到上面的两类, 然后在隐向量维度上拼接起来, 组成一个更长的向量进行 attention 计算. 相当于每个参与 attention 计算的向量, 部分参数包含 RoPE 位置信息, 另外的参数则不包含.\n具体来说, 上面使用 Low-Rank compression 得到的 $\\mathbf{k}_t^C$ 和 $\\mathbf{q}_t^C$ 是不需要计算 RoPE 的 queries 和 keys, 相应的维度为 $d_h n_h$. 另外设置需要计算 RoPE 的 queries 和 keys $\\mathbf{q}_{t,i}^{R} \\in \\mathbb{R}^{d_h^R}$ 和 $\\mathbf{k}_t^R \\in \\mathbb{R}^{d_h^R}$, 其中 $i$ 表示第 $i$ 个 head, $d_h^R$ 代表了这部分每个 head 中的维度.\n在使用了上面的方法, 将 MLA 与 RoPE 结合后, 对应的计算过程为:\n$$ \\begin{aligned} \u0026amp; {\\left[\\mathbf{q}_{t, 1}^R ; \\mathbf{q}_{t, 2}^R ; \\ldots ; \\mathbf{q}_{t, n_h}^R\\right]=\\mathbf{q}_t^R=\\operatorname{RoPE}\\left(W^{Q R} \\mathbf{c}_t^Q\\right),} \\\\ \u0026amp; \\mathbf{k}_t^R=\\operatorname{RoPE}\\left(W^{K R} \\mathbf{h}_t\\right) \\text {, } \\\\ \u0026amp; \\mathbf{q}_{t, i}=\\left[\\mathbf{q}_{t, i}^C ; \\mathbf{q}_{t, i}^R\\right] \\text {, } \\\\ \u0026amp; \\mathbf{k}_{t, i}=\\left[\\mathbf{k}_{t, i}^C ; \\mathbf{k}_t^R\\right] \\text {, } \\\\ \u0026amp; \\mathbf{o}_{t, i}=\\sum_{j=1}^t \\operatorname{Softmax}_j\\left(\\frac{\\mathbf{q}_{t, i}^T \\mathbf{k}_{j, i}}{\\sqrt{d_h+d_h^R}}\\right) \\mathbf{v}_{j, i}^C \\\\ \u0026amp; \\mathbf{u}_t=W^O\\left[\\mathbf{o}_{t, 1} ; \\mathbf{o}_{t, 2} ; \\ldots ; \\mathbf{o}_{t, n_h}\\right], \\end{aligned} $$\n可以看到, 每个 head 中计算 RoPE 的 query $\\mathbf{q}_{t,i}^{R}$ 和 $\\mathbf{k}_t^R$ 的维度都是 $d_h^R$, 而 query 是每个 head 都有, 但 key 是所有 heads 中的 queries 中共享同一个 $\\mathbf{k}_t^R$, 是标准的 MQA 思路.\n$\\mathbf{q}_{t, i}^C ; \\mathbf{q}_{t, i}^R$ 两种 query 拼接在一起得到新的 query, $\\mathbf{k}_{t, i}^C ; \\mathbf{k}_t^R$ 两种 key 拼接在一起得到新的 key, 也能明显看到, key 的一部分是 MHA 的思路, 使用了 Low-Rank compression, 另一部分是所有 head 中拼接相同的 key, 使用了 MQA 的思路.\nKV Cache 最终大小 从上面两节可以看到, MLA 是一种 MHA 和 MQA 之间的均衡:\nMHA 部分对应的 $\\mathbf{k}_t^C \\in \\mathbb{R}^{d_h n_h}$ 在上面分析了, 缓存的是 $\\mathbf{c}_t^{K V}$, $\\mathbf{c}_t^{K V}$ 其实也是所有 head 共用的, 是经过 $W^{U K}$ 得到了 attention 计算各个 head 对应的 keys $\\mathbf{k}_t^C = \\left[\\mathbf{k}_{t, 1}^R ; \\mathbf{k}_{t, 2}^R ; \\ldots ; \\mathbf{k}_{t, n_h}^R\\right]$ MQA 部分引入的 $\\mathbf{k}_t^R$ 是需要被缓存的, 这部分包含了 RoPE 的位置信息 因此整个 MLA 机制需要缓存的内容为 $\\mathbf{c}_t^{K V} \\in \\mathbb{R}^{d_c}$ 和 $\\mathbf{k}_t^R \\in \\mathbb{R}^{d_h^R}$, 这样每个 token 对应的 KV Cache 为 $(d_c + d_h^{R})l$ 个元素.\n下图是 MHA, GQA, MQA, MLA 四种 attention 机制的 KV Cache 的对比. 参考开源的 DeepSeek-V2 模型中的 config, 可以得到 heads 数量为 $n_h = 128$, head_dim $d_h=128$. 而 MLA 对应的参数为 $d_c=512$, $d_h^R=64$. 可以得到 MLA 中每个 token 对应的 cache 大小为 $(512 + 64)l = \\frac{9}{2} \\times 128 l = \\frac{9}{2} d_h l$, 这个大小介于 MHA 的 $256 d_h l$ 与 MQA 的 $2 d_h l$ 之间. 与 GQA 相比, 相当与 group 数量 $n_g = 2.25$ 大小.\n消融实验设计了两组, 分别是 MHA, GQA, MQA 这三种传统方案的对比, 以及 MHA 和 MLA 这两种方案的对比.\n在 MHA, GQA, MQA 对比的消融实现中, 使用 7B 大小的模型, 在 1.33T tokens 上分别进行训练得到. 下表中很明显, MHA 是显著由于另外两种方案.\n再对比 MHA 和 MLA. 结合 MoE 架构, 在 16B 总参数规模和 250B 总参数规模进行了实验, 对比结果如下表, 在显著降低 KV Cache 的情况下(Small MoE 中 MLA 的 cache 大小为 MHA 的 14%, Large MoE 中为 4%), 基本上在各个 benchmark 上, MLA 都明显超越了 MHA.\nMLA 的完整过程 MLA 中, 第 $t$ 个 token 在 attention 结构中完整的计算过程如下:\n$$ \\begin{aligned} \u0026amp; \\mathbf{c}_t^Q=W^{D Q} \\mathbf{h}_t, \\\\ \u0026amp; {\\left[\\mathbf{q}_{t, 1}^C ; \\mathbf{q}_{t, 2}^C ; \\ldots ; \\mathbf{q}_{t, n_h}^C\\right]=\\mathbf{q}_t^C=W^{U Q} \\mathbf{c}_t^Q ，} \\\\ \u0026amp; {\\left[\\mathbf{q}_{t, 1}^R ; \\mathbf{q}_{t, 2}^R ; \\ldots ; \\mathbf{q}_{t, n_h}^R\\right]=\\mathbf{q}_t^R=\\operatorname{RoPE}\\left(W^{Q R} \\mathbf{c}_t^Q\\right) \\text {, }} \\\\ \u0026amp; \\mathbf{q}_{t, i}=\\left[\\mathbf{q}_{t, i}^C ; \\mathbf{q}_{t, i}^R\\right] \\text {, } \\\\ \u0026amp; \\mathbf{c}_t^{K V}=W^{D K V} \\mathbf{h}_t \\text {, } \\\\ \u0026amp; {\\left[\\mathbf{k}_{t, 1}^C ; \\mathbf{k}_{t, 2}^C ; \\ldots ; \\mathbf{k}_{t, n_h}^C\\right]=\\mathbf{k}_t^C=W^{U K} \\mathbf{c}_t^{K V},} \\\\ \u0026amp; \\mathbf{k}_t^R=\\operatorname{RoPE}\\left(W^{K R} \\mathbf{h}_t\\right), \\\\ \u0026amp; \\mathbf{k}_{t, i}=\\left[\\mathbf{k}_{t, i}^C ; \\mathbf{k}_t^R\\right], \\\\ \u0026amp; {\\left[\\mathbf{v}_{t, 1}^C ; \\mathbf{v}_{t, 2}^C ; \\ldots ; \\mathbf{v}_{t, n_h}^C\\right]=\\mathbf{v}_t^C=W^{U V} \\mathbf{c}_t^{K V},} \\\\ \u0026amp; \\mathbf{o}_{t, i}=\\sum_{j=1}^t \\operatorname{Softmax}_j\\left(\\frac{\\mathbf{q}_{t, i}^T \\mathbf{k}_{j, i}}{\\sqrt{d_h+d_h^R}}\\right) \\mathbf{v}_{j, i^{\\prime}}^C \\\\ \u0026amp; \\mathbf{u}_t=W^O\\left[\\mathbf{o}_{t, 1} ; \\mathbf{o}_{t, 2} ; \\ldots ; \\mathbf{o}_{t, n_h}\\right], \\end{aligned} $$\n对应的流程图如下(这图真的清晰, 每个张量的 head 情况都准确描述):\n代码 具体的细节还是要看代码. 从上面的公式中很明显可以看到引入了很多的权重参数$W$. 首先我们先定义下各个大小参数, 结合开源的 DeepSeek-V2 模型中的 config 中的数值:\nhidden_size: 本层输入的大小, 对应上面的 $d$, 值为 5120 num_heads: head 数量, 对应 $n_h$, 值为 128 q_lora_rank: queries 进行 low-rank compression 后的维度, 对应 $d_c^{\\prime}$, 值为 1536 qk_nope_head_dim: query 中进行 low-rank compression 不需要进行 RoPE 的维度, 相当于原始的 head 内的维度, 对应 $d_h$, 值为 128 qk_rope_head_dim: query 中进行 RoPE 的维度, 对应 $d_h^R$, 值为 64 q_head_dim: query 拼接后的维度, 对应 $d_h + d_h^R$, 值为 128 + 64 kv_lora_rank: keys 进行 low-rank compression 后的维度, 对应 $d_c$, 值为 512 v_head_dim: values 对应的维度, 即原始的 head 内的维度, 对应 $d_h$, 值为 128 在代码中, 关于 query 和 key 这部分定义了 4 个参数权重变量, MLA 引入的各种权重参数, 可以看做通过各种拼接组合, 最终形成了这 4 个变量:\nq_a_proj: (hidden_size, q_lora_rank), 即 $(d, d_c^{\\prime})$. 这个权重对应的就是公式中的 $W^{DQ} \\in \\mathbb{R}^{d \\times d_c^{\\prime}}$, 将输入 $\\mathbf{h}_t$ 映射为低秩压缩后的 $\\mathbf{c}_t^Q$ q_b_proj: (q_lora_rank, num_heads * q_head_dim), 即 $(d_c^{\\prime}, n_h \\times (d_h + d_h^R))$, 对应的权重为 $W^{UQ} \\in \\mathbb{R}^{n_h d_h \\times d_c^{\\prime}}$ 和 $W^{QR} \\in \\mathbb{R}^{n_h d_h \\times d_h^{R}}$, 两个权重分别将低秩压缩后的 $\\mathbf{c}_t^Q$ 转化为每个 heads 中不同的两种 queries 成分, 部分经过 RoPE 后再拼接得到最终的 queries kv_a_proj_with_mqa: (hidden_size, kv_lora_rank + qk_rope_head_dim), 即 $(d, d_c + d_h^R)$, 对应的权重为 $W^{DKV} \\in \\mathbb{R}^{d \\times d_c}$ 和 $W^{KR} \\in \\mathbb{R}^{d \\times d_h^R}$, 两个权重分别将输入 $\\mathbf{h}_t$ 转化为公共的两种 cache, $\\mathbf{c}_t^{K V} \\in \\mathbb{R}^{d_c}$ 和 $\\mathbf{k}_t^R \\in \\mathbb{R}^{d_h^R}$ kv_b_proj: (kv_lora_rank, num_heads * (qk_nope_head_dim + v_head_dim)), 即 $(d_c, n_h \\times (d_h + d_h))$, 对应的权重为 $W^{UK} \\in \\mathbb{R}^{d_c \\times n_h d_h}$ 和 $W^{UV} \\in \\mathbb{R}^{d_c \\times n_h d_h}$, 其中 $W^{UK}$ 是将不进行 RoPE 的 keys 部分还原出来, 作为每个 heads 中的 keys 的一部分成分, 与上面得到的 $\\mathbf{k}_t^R$ 拼接得到最终的 keys; $W^{UV}$ 是将 KV cache 中的 values 还原出来 使用到的矩阵 相比于 MHA 用到的三个 $(d, n_h d_h)$ 矩阵, 将输入 $\\mathbf{h}_t$ 转换成 queries, keys, values, 由于降秩压缩的存在, attention 结构的参数量有减少. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 self.hidden_size = config.hidden_size # 5120 self.num_heads = config.num_attention_heads # 128 self.q_lora_rank = config.q_lora_rank # 1536 self.qk_rope_head_dim = config.qk_rope_head_dim # 64 self.qk_nope_head_dim = config.qk_nope_head_dim # 128 self.q_head_dim = self.qk_rope_head_dim + self.qk_nope_head_dim # 192 self.kv_lora_rank = config.kv_lora_rank # 512 self.v_head_dim = config.v_head_dim # 128 # q_a_proj: (5120, 1536) self.q_a_proj = nn.Linear( self.hidden_size, config.q_lora_rank, bias=config.attention_bias, ) # q_b_proj: (1536, 128 * 192 = 24576) self.q_b_proj = nn.Linear( config.q_lora_rank, self.num_heads * self.q_head_dim, bias=False, ) # kv_a_proj_with_mqa: (5120, 512 + 64 = 576) self.kv_a_proj_with_mqa = nn.Linear( self.hidden_size, config.kv_lora_rank + config.qk_rope_head_dim, bias=config.attention_bias, ) # kv_b_proj: (512, 128 * (192 - 64 + 128) = 32768) self.kv_b_proj = nn.Linear( config.kv_lora_rank, self.num_heads * (self.q_head_dim - self.qk_rope_head_dim + self.v_head_dim), bias=False, ) # o_proj: (128 * 128 = 16384, 5120) self.o_proj = nn.Linear( self.num_heads * self.v_head_dim, self.hidden_size, bias=config.attention_bias, ) 前向传播过程的代码如下, 可以与公式中的过程一一对应.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 \u0026#34;\u0026#34;\u0026#34;queries 的处理\u0026#34;\u0026#34;\u0026#34; # t1: hidden_states: (b, s, 5120) # t2: self.q_a_proj(hidden_states): (b, s, 1536) # t3: self.q_a_layernorm(t2): (b, s, 1536) # q: t4: self.q_b_proj(t3): (b, s, 128 * 192) q = self.q_b_proj(self.q_a_layernorm(self.q_a_proj(hidden_states))) # t1: q: (b, s, 128 * 192) # t2: q.view: (b, s, 128, 192) # q: t3: t2.transpose(1, 2): (b, 128, s, 192 = (128 + 64)) q = q.view(bsz, q_len, self.num_heads, self.q_head_dim).transpose(1, 2) # q_nope: (b, 128, s, 128) # q_pe: (b, 128, s, 64) q_nope, q_pe = torch.split( q, [self.qk_nope_head_dim, self.qk_rope_head_dim], dim=-1 ) \u0026#34;\u0026#34;\u0026#34;keys and values 的处理\u0026#34;\u0026#34;\u0026#34; # t1: hidden_states: (b, s, 5120) # compressed_kv: t2: self.kv_a_proj_with_mqa(t1): (b, s, 576 = 512 + 64) compressed_kv = self.kv_a_proj_with_mqa(hidden_states) # compressed_kv: (b, s, 512) # k_pe: (b, s, 64) compressed_kv, k_pe = torch.split( compressed_kv, [self.kv_lora_rank, self.qk_rope_head_dim], dim=-1 ) # t1: k_pe: (b, s, 64) # t2: k_pe.view: (b, s, 1, 64) # k_pe: t2.transpose(1, 2): (b, 1, s, 64) k_pe = k_pe.view(bsz, q_len, 1, self.qk_rope_head_dim).transpose(1, 2) # t1: compressed_kv: (b, s, 512) # t2: self.kv_a_layernorm(t1): (b, s, 512) # t3: self.kv_b_proj(t2): (b, s, 128 * (128 + 128)) # t4: t3.view: (b, s, 128, 128 + 128) # kv: t5: t4.transpose(1, 2): (b, 128, s, 128 + 128) kv = ( self.kv_b_proj(self.kv_a_layernorm(compressed_kv)) .view(bsz, q_len, self.num_heads, self.qk_nope_head_dim + self.v_head_dim) .transpose(1, 2) ) \u0026#34;\u0026#34;\u0026#34;对部分 queries 和 keys 施加 RoPE\u0026#34;\u0026#34;\u0026#34; # k_nope: (b, 128, s, 128) # value_states: (b, 128, s, 128) k_nope, value_states = torch.split( kv, [self.qk_nope_head_dim, self.v_head_dim], dim=-1 ) # kv_seq_len: s kv_seq_len = value_states.shape[-2] cos, sin = self.rotary_emb(value_states, seq_len=kv_seq_len) # q_pe: (b, 128, s, 64) # k_pe: (b, 1, s, 64) q_pe, k_pe = apply_rotary_pos_emb(q_pe, k_pe, cos, sin, position_ids) \u0026#34;\u0026#34;\u0026#34;将经过 RoPE 的 queries 和 keys 分别于不需要经过 RoPE 的部分拼接 得到新的 queries 和 keys \u0026#34;\u0026#34;\u0026#34; # query_states: (b, 128, s, 128 + 64) query_states = k_pe.new_empty(bsz, self.num_heads, q_len, self.q_head_dim) # query_states 的 (b, 128, s, :128) 部分拼接不需要进行 rope 的部分 query_states[:, :, :, : self.qk_nope_head_dim] = q_nope # query_states 的 (b, 128, s, 128:) 部分拼接进行了 rope 的部分 query_states[:, :, :, self.qk_nope_head_dim :] = q_pe # key_states: (b, 128, s, 128 + 64) key_states = k_pe.new_empty(bsz, self.num_heads, q_len, self.q_head_dim) # key_states 的 (b, 128, s, :128) 部分拼接不需要进行 rope 的部分 key_states[:, :, :, : self.qk_nope_head_dim] = k_nope # key_states 的 (b, 128, s, 128:) 部分拼接进行了 rope 的部分 key_states[:, :, :, self.qk_nope_head_dim :] = k_pe \u0026#34;\u0026#34;\u0026#34;计算 attention\u0026#34;\u0026#34;\u0026#34; # t1: query_states: (b, 128, s, 192) # t2: key_states.transpose(2, 3): (b, 128, 192, s) # attn_weights: matmul(t1, t2): (b, 128, s, s) attn_weights = ( torch.matmul(query_states, key_states.transpose(2, 3)) * self.softmax_scale ) # 计算 attention softmax 时 upcast 到 fp32 # attn_weights: (b, 128, s, s) attn_weights = nn.functional.softmax( attn_weights, dim=-1, dtype=torch.float32 ).to(query_states.dtype) attn_weights = nn.functional.dropout( attn_weights, p=self.attention_dropout, training=self.training ) \u0026#34;\u0026#34;\u0026#34;得到最终的输出\u0026#34;\u0026#34;\u0026#34; # t1: attn_weights: (b, 128, s, s) # t2: value_states: (b, 128, s, 128) # attn_output: matmul(t1, t2): (b, 128, s, 128) attn_output = torch.matmul(attn_weights, value_states) # t1: attn_output: (b, 128, s, 128) # attn_output: t2: t1.transpose(1, 2): (b, s, 128, 128) attn_output = attn_output.transpose(1, 2).contiguous() # t1: attn_output: (b, s, 128, 128) # attn_output: t2: t1.reshape: (b, s, 128 * 128) attn_output = attn_output.reshape(bsz, q_len, self.num_heads * self.v_head_dim) # t1: attn_output: (b, s, 128 * 128) # attn_output: self.o_proj(t1): (b, s, 5120) attn_output = self.o_proj(attn_output) MoE 在 FFN 层中, 使用了 DeepSeekMoE 结构. DeepSeekMoE 中将 experts 划分为了两类:\nRouted Expert: 这部分专家负责更准确地获取专业知识 Shared Expert: 这部分专家负责减少 Routed Expert 之间的知识冗余情况 对于每一个 token, 所有的 Shared Expert 都会被激活, 除此之外再根据 gate 选择 TopK 的 Routed Expert 激活, 将这些专家的输出汇总得到这个 token 最终的输出. 整个过程如下图所示.\n$$ \\begin{aligned} \u0026amp; \\mathbf{h}_t^{\\prime}=\\mathbf{u}_t+\\sum_{i=1}^{N_s} \\operatorname{FFN}_i^{(s)}\\left(\\mathbf{u}_t\\right)+\\sum_{i=1}^{N_r} g_{i, t} \\operatorname{FFN}_i^{(r)}\\left(\\mathbf{u}_t\\right), \\\\ \u0026amp; g_{i, t} = \\begin{cases} s_{i, t}, \u0026amp; s_{i, t} \\in \\operatorname{Topk}\\left(\\left\\{s_{j, t} \\mid 1 \\leqslant j \\leqslant N_r\\right\\}, K_r\\right), \\\\ 0, \\quad \\text { otherwise, }\\end{cases} \\\\ \u0026amp; s_{i, t}=\\operatorname{Softmax}_i\\left(\\mathbf{u}_t^T \\mathbf{e}_i\\right), \\end{aligned} $$\n公式化表述:\n$\\mathbf{u}_t$ 是第 $t$ 个 token 的 FFN 输入, $N_s$ 和 $N_r$ 分别代表 Shared Expert 和 Routed Expert 的数量, $\\operatorname{FFN}_i^{(s)}(\\cdot)$ 和 $\\operatorname{FFN}_i^{(r)}(\\cdot)$ 分别代表了第 $i$ Shared Expert 和第 $i$ 个 Routed Expert.\n$K_r$ 代表了要激活几个 Routed Expert. $g_{i,t}$ 代表了第 $i$ 个 Routed Expert 对应的 gate value.\n$s_{i,t}$ 代表了当前第 $t$ 个 token 对第 $i$ 个 Routed Expert 的倾向程度, 由每个 Routed Expert 对应的 softmax 得到. $\\mathbf{e}_i$ 是这层中第 $i$ 个 Routed Expert 对应的 centroid 向量.\n最后将选择出的 TopK Routed Expert 乘上对应的 $g_{i,t}$ gate value 作为权重, 再加上所有 Shared Expert 的输入, 以及原始输入 $\\mathbf{u}_t$, 得到第 $t$ 个 token 的最终输出 $\\mathbf{h}_t^{\\prime}$.\n在开源的 DeepSeek-V2 模型中的 config 中对应的 Routed Expert n_routed_experts 数量为 160, 选取 num_experts_per_tok TopK $K_r = 6$; Shared Expert 的数量 n_shared_experts 为 2.\n代码 其中还是有很多细节, 需要代码阐释. 首先是 MoE 的整体是怎么定义的. 其中通过 config.ep_size 的配置定义了 expert parallel 的数量, 即针对 expert 是否并行计算, DeepSeekMoE 结构进行了专门的优化.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 class DeepseekV2MoE(nn.Module): \u0026#34;\u0026#34;\u0026#34; A mixed expert module containing shared experts. \u0026#34;\u0026#34;\u0026#34; def __init__(self, config): super().__init__() self.config = config self.num_experts_per_tok = config.num_experts_per_tok if hasattr(config, \u0026#34;ep_size\u0026#34;) and config.ep_size \u0026gt; 1: ... else: ... 我们只看非并行部分, 作为理解. num_experts_per_tok 定义了每个 token 用几个 router expert. Router experts 中实际上为一个个 DeepseekV2MLP, 这个就是使用了 SwiGLU 激活函数的 MLP 结构.\n1 2 3 4 5 6 7 8 9 10 11 12 if hasattr(config, \u0026#34;ep_size\u0026#34;) and config.ep_size \u0026gt; 1: ... else: self.ep_size = 1 self.experts_per_rank = config.n_routed_experts self.ep_rank = 0 self.experts = nn.ModuleList( [ DeepseekV2MLP(config, intermediate_size=config.moe_intermediate_size) for i in range(config.n_routed_experts) ] ) 然后定义了 MoE 需要的 Gate 结构, 后面展开. 然后定义了 Shared experts. 可以看到是使用了一个大的 DeepseekV2MLP 结构作为所有 Shared experts 的汇总. 如果 shared experts 的数量增加, MLP 结构的 intermediate_size 会成比例的增大.\n1 2 3 4 5 6 self.gate = MoEGate(config) if config.n_shared_experts is not None: intermediate_size = config.moe_intermediate_size * config.n_shared_experts self.shared_experts = DeepseekV2MLP( config=config, intermediate_size=intermediate_size ) 计算过程. 首先接收 hidden_states 作为输入. 将输入传入到 gate 结构中, 得到 topK router experts 对应的 index topk_idx, 以及对应的权重分数 topk_weight.\n1 2 3 4 def forward(self, hidden_states): identity = hidden_states orig_shape = hidden_states.shape topk_idx, topk_weight, aux_loss = self.gate(hidden_states) MoEGate 在 MoEGate 中, 一个 batch 内, 每个样本中的每个 token 都会选择出它对应的 topk 的 router experts. MoEGate 初始化, 声明了一个可训练的 centroid 参数矩阵 weight, 每个 router expert 对应一个向量, 因此 centroid 矩阵的大小为 (n_routed_experts, gating_dim), gating_dim 实际上为 hidden_size 大小.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 class MoEGate(nn.Module): def __init__(self, config): super().__init__() self.config = config self.top_k = config.num_experts_per_tok self.n_routed_experts = config.n_routed_experts self.routed_scaling_factor = config.routed_scaling_factor self.scoring_func = config.scoring_func self.alpha = config.aux_loss_alpha self.seq_aux = config.seq_aux self.topk_method = config.topk_method self.n_group = config.n_group self.topk_group = config.topk_group # topk selection algorithm self.norm_topk_prob = config.norm_topk_prob self.gating_dim = config.hidden_size self.weight = nn.Parameter( torch.empty((self.n_routed_experts, self.gating_dim)) ) self.reset_parameters() 在前向传播阶段, 输入 hidden_states 为 (b, s, hidden_size), 将输入通过 weight 映射为 (b, s, n_routed_experts), 即 batch 内, 每个样本中的每个 token 对应在所有 router expert 的 logits 分数, 之后进行 softmax 得到归一化的分数.\n1 2 3 4 5 6 7 8 9 10 11 12 13 def forward(self, hidden_states): bsz, seq_len, h = hidden_states.shape ### compute gating score hidden_states = hidden_states.view(-1, h) logits = F.linear( hidden_states.type(torch.float32), self.weight.type(torch.float32), None ) if self.scoring_func == \u0026#34;softmax\u0026#34;: scores = logits.softmax(dim=-1, dtype=torch.float32) else: raise NotImplementedError( f\u0026#34;insupportable scoring function for MoE gating: {self.scoring_func}\u0026#34; ) 之后挑选出 topk 的 experts 和对应的分数. 这里有两种策略: gready 和 group_limited_greedy. gready 的方法很简单, 直接使用 topk 函数, 选出 batch 内每个 token 对应的 topk index 以及对应的分数 topk_idx 和 topk_weight, 两个张量的维度为 (b, s, top_k).\n1 2 3 4 5 6 7 ### select top-k experts if self.topk_method == \u0026#34;gready\u0026#34;: topk_weight, topk_idx = torch.topk( scores, k=self.top_k, dim=-1, sorted=False ) elif self.topk_method == \u0026#34;group_limited_greedy\u0026#34;: ... 而根据配置文件, deepseek-v2 默认的方法为 group_limited_greedy. 在这种方法下, router experts 不再是完全平等竞争, 而是分成了几组, 首先每个组选出代表, 组间进行竞争, 淘汰点所有落后组及其对应的 experts, 再在剩余的组中选择出 topk 的 experts. 在 DeepSeek-V2 的 config 中, 分成了 n_group = 8 组, 总共要选出 topk_group = 3 个组, 然后在这 3 个组的所有 experts 中选举组最优的 num_experts_per_tok = 6 个 experts.\n分组的方法, 是将最后一维 n_routed_experts 通过 view 方法转换成 (n_group, n_routed_experts // n_group), 并且将 (b, s) 碾平, 记 b * s = n. 首先得到每个 token 每个 group 的最大分数 group_scores: (n, n_group).\n然后根据分数, 选取每个 token 对应的 topk 个组的 index group_idx: (n, topk_group).\n然后根据得到的 index 创建 group 的 mask 矩阵, 矩阵的大小为 (n, n_group), 值为 1 代表这个 group 是被选择为 top group 的组, 为 0 代表非 top group, 后面不再使用. 这一步是通过 scatter_ 方法实现的, 得到 group_mask: (n, n_group).\n之后, 要根据得到的 top group, 将这些组内所有的 expert 选择出来, 后面在做比较. 方法是根据得到的 group_mask 得到所有 expert 对应的 expert mask, 这里是 score_mask: (n, n_routed_experts). 方法是将 group_mask 扩展 n_routed_experts // n_group 遍, 这样 group_mask 的值就映射到了组内所有的 expert 上, 然后将 mask 矩阵的形状调整为需要的形式. 使用了 unsqueeze, expand, reshape 方法.\n得到了 expert mask 之后, 将这个 mask 矩阵覆盖到之前得到的每个 expert 的分数, 挑选出 top group 对应的所有 experts, 在选出这其中的 topk 个 experts.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 elif self.topk_method == \u0026#34;group_limited_greedy\u0026#34;: group_scores = ( scores.view(bsz * seq_len, self.n_group, -1).max(dim=-1).values ) # [n, n_group] group_idx = torch.topk( group_scores, k=self.topk_group, dim=-1, sorted=False )[ 1 ] # [n, top_k_group] group_mask = torch.zeros_like(group_scores) # [n, n_group] group_mask.scatter_(1, group_idx, 1) # [n, n_group] score_mask = ( group_mask.unsqueeze(-1) .expand( bsz * seq_len, self.n_group, self.n_routed_experts // self.n_group ) .reshape(bsz * seq_len, -1) ) # [n, e] tmp_scores = scores.masked_fill(~score_mask.bool(), 0.0) # [n, e] topk_weight, topk_idx = torch.topk( tmp_scores, k=self.top_k, dim=-1, sorted=False ) 将 topk 对应的 weights 进行归一化调整.\n1 2 3 4 5 6 ### norm gate to sum 1 if self.top_k \u0026gt; 1 and self.norm_topk_prob: denominator = topk_weight.sum(dim=-1, keepdim=True) + 1e-20 topk_weight = topk_weight / denominator else: topk_weight = topk_weight * self.routed_scaling_factor 接下来, DeepSeek-V2 引入了 Expert-Level Balance Loss 的概念, 用在训练阶段, 目的是避免 routing collapse 现象, 即所有 tokens 只激活一部分 experts, 另外一部分 experts 很少被激活, 导致参数效率低下.\nExpert-Level Balance Loss 的定义为\n$$ \\begin{aligned} \\mathcal{L}_{\\mathrm{ExpBal}}\u0026amp; =\\alpha_{1}\\sum_{i=1}^{N_{r}}f_{i}P_{i}, \\\\ \u0026amp;f_{i} =\\frac{N_{r}}{K_{r}T}\\sum_{t=1}^{T}\\mathbb{1}(\\mathrm{Token~}t\\mathrm{~selects~Expert~}i), \\\\ \u0026amp;P_{i} =\\frac{1}{T}\\sum_{t=1}^{T}s_{i,t}, \\end{aligned} $$\n$s_{i,t}$ 代表的就是第 $i$ 个 router expert 在第 $t$ 个 token 上的分数, 将所有 token 的分数平均, 得到这个 expert 对应的分数 $P_i$. $f_i$ 是第 $i$ 个 expert 对应的 loss 权重, 与 batch 内有多少个 token 激活了这个 expert 相关, 越多的 tokens 选择这个 expert, 则这个 expert 产生的 loss 越大.\n最后将分数 $P_i$ 和权重 $f_i$ 乘在一起, expert 的得分越高, 或者这个选择 expert 的 tokens 越多, 说明这个 expert 越有被偏向的风险. 通过 Expert-Level Balance Loss 这个正则项, 来缓解这个问题.\n$\\alpha_1$ 是这个 loss 的平衡参数.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 ### expert-level computation auxiliary loss if self.training and self.alpha \u0026gt; 0.0: scores_for_aux = scores aux_topk = self.top_k # always compute aux loss based on the naive greedy topk method topk_idx_for_aux_loss = topk_idx.view(bsz, -1) if self.seq_aux: scores_for_seq_aux = scores_for_aux.view(bsz, seq_len, -1) ce = torch.zeros( bsz, self.n_routed_experts, device=hidden_states.device ) ce.scatter_add_( 1, topk_idx_for_aux_loss, torch.ones(bsz, seq_len * aux_topk, device=hidden_states.device), ).div_(seq_len * aux_topk / self.n_routed_experts) aux_loss = (ce * scores_for_seq_aux.mean(dim=1)).sum( dim=1 ).mean() * self.alpha else: mask_ce = F.one_hot( topk_idx_for_aux_loss.view(-1), num_classes=self.n_routed_experts ) ce = mask_ce.float().mean(0) Pi = scores_for_aux.mean(0) fi = ce * self.n_routed_experts aux_loss = (Pi * fi).sum() * self.alpha else: aux_loss = None MoE forward 1 2 3 4 5 6 7 8 9 10 11 12 13 14 def forward(self, hidden_states): identity = hidden_states orig_shape = hidden_states.shape topk_idx, topk_weight, aux_loss = self.gate(hidden_states) hidden_states = hidden_states.view(-1, hidden_states.shape[-1]) flat_topk_idx = topk_idx.view(-1) if self.training: ... else: ... if self.config.n_shared_experts is not None: y = y + self.shared_experts(identity) return y 接下来产生了训练阶段和推理阶段的分歧. 首先看训练阶段.\nhidden_states 转换为 (b * s, hidden_size). 首先将其重复 num_experts_per_tok 次, 得到 (b * s * num_experts_per_tok, hidden_size). flat_topk_idx: (b * s * num_experts_per_tok,) 由 topk_idx 转换得来.\nMoE 结构最后的输出形状与输入 hidden_states 一样, 这里先创建一个空的最终输出 y: (b * s * num_experts_per_tok, hidden_size). 然后遍历每个 router expert:\nflat_topk_idx == i 选择出所有激活这个 expert 的索引, 记为 t1 由于 flat_topk_idx 的大小为 b * s * num_experts_per_tok, hidden_states 的第一个维度也是 b * s * num_experts_per_tok, 所以通过 hidden_states[t1] 可以选择出需要进入到这个 expert 中所有 token 对应输入的拼接, 记为 t2 将选择出的 t2 输入到对应的 expert, 其实就是一个 MLP 层, 得到对应的输出, 按照对应的位置放置到对应的位置: y[flat_topk_idx == i] = expert(t2) 循环完所有的 expert, 就得到了所有 token 选择的 expert 的输出 y: (b * s * num_experts_per_tok, hidden_size).\n每个 token 对应的 MoE 输出, 是加权汇总所有 top router expert 的输出, 汇总的权重是 MoEGate 输出的权重分数 topk_weight: (b * s, num_experts_per_tok), 将 y 与 topk_weight 相乘, 并 sum 就得到了每个 token 的 MoE 输出. 再经过形状的变换, 得到输出 y: (b, s, hidden_size).\n这里的 AddAuxiliaryLoss 是将 MoEGate 计算出的 Expert-Level Balance Loss 通过特殊的 trick 注册到计算过程中.\n1 2 3 4 5 6 7 8 9 10 if self.training: hidden_states = hidden_states.repeat_interleave( self.num_experts_per_tok, dim=0 ) y = torch.empty_like(hidden_states) for i, expert in enumerate(self.experts): y[flat_topk_idx == i] = expert(hidden_states[flat_topk_idx == i]) y = (y.view(*topk_weight.shape, -1) * topk_weight.unsqueeze(-1)).sum(dim=1) y = y.view(*orig_shape) y = AddAuxiliaryLoss.apply(y, aux_loss) 再看推理阶段.\n首先统计每个 expert 有多少个 tokens 选择.\n1 2 3 4 5 6 # (b * s, n_routed_experts) cnts = topk_ids.new_zeros((topk_ids.shape[0], len(self.experts))) # (b * s, n_routed_experts) cnts.scatter_(1, topk_ids, 1) # (n_routed_experts,) tokens_per_expert = cnts.sum(dim=0) 然后整合所有 token 选择的 expert 对应的输出, 由于有 b * s 个 token, 每个选择 num_experts_per_tok 个 expert, 因此最终得到一个 (b * s * num_experts_per_tok, hidden_size) 的变量 sorted_tokens.\n为了后续计算的方便, 需要将选择相同的 expert 拼接不同 tokens 在一起. 这里的 trick 是将代表选择的 top index topk_ids 碾平后进行 argsort, 对 argsort 整除 n_routed_experts 就得到了对应的 token index. 这样通过 x[idxs // n_routed_experts] 即完成了选择相同的 expert 的 tokens 输入拼接在一起(index 低的 expert 拼接后排在前).\n1 2 3 4 5 # (b * s * num_experts_per_tok,) idxs = topk_ids.view(-1).argsort() # (b * s * num_experts_per_tok, hidden_size) sorted_tokens = x[idxs // topk_ids.shape[1]] sorted_tokens_shape = sorted_tokens.shape 循环每个 expert, 首先从 tokens_per_expert 得到这个 expert 对应的激活的 tokens 的数量 num_tokens. 而由于 sorted_tokens 是选择相同的 expert 的 tokens 输入拼接在一起的大的输入矩阵, 且按 expert index 的顺序排列, 低 index 的 expert 排名在 sorted_tokens 的前面. 所以通过 start_idx: start_idx + num_tokens 就可以得到这个 expert 对应的所有 tokens 的输入.\n1 2 3 4 for i, num_tokens in enumerate(tokens_per_expert): end_idx = start_idx + num_tokens if num_tokens == 0: continue 将这部分输入通过 slice 截取出来, 并经过对应的 expert 的 MLP 得到对应的输出.\n1 2 3 4 5 expert = self.experts[i + self.ep_rank * self.experts_per_rank] tokens_for_this_expert = sorted_tokens[start_idx:end_idx] expert_out = expert(tokens_for_this_expert) outputs.append(expert_out) start_idx = end_idx 循环所有的 expert 后, 将每个 expert 的结果拼接起来, 得到所有 tokens 在其选择的 experts 的所有输出拼接在一起.\n1 2 # outs: (b * s * num_experts_per_tok, hidden_size) outs = torch.cat(outputs, dim=0) if len(outputs) else sorted_tokens.new_empty(0) 接下来, 要对每个 token 进行汇总, 加权每个 token 在它选择的所有 experts 上的输出. idxs 记录了 token 和 num_experts_per_tok 交叉后的顺序 index, new_x 经过 view 后的形状为 (b * s, num_experts_per_tok, hidden_size). 在乘上 topk_weight 后, 通过 sum 加权汇总 experts 的输出, 得到最终每个 token 的输出 final_out: (b * s, hidden_size)\n1 2 3 4 5 6 7 8 9 10 11 # idxs: # (b * s * num_experts_per_tok,) new_x = torch.empty_like(outs) new_x[idxs] = outs final_out = ( new_x.view(*topk_ids.shape, -1) .type(topk_weight.dtype) .mul_(topk_weight.unsqueeze(dim=-1)) .sum(dim=1) .type(new_x.dtype) ) 完成的推理代码参考:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 # x: (b * s, hidden_size) # topk_ids: (b * s, n_routed_experts) @torch.no_grad() def moe_infer(self, x, topk_ids, topk_weight): # (b * s, n_routed_experts) cnts = topk_ids.new_zeros((topk_ids.shape[0], len(self.experts))) # (b * s, n_routed_experts) cnts.scatter_(1, topk_ids, 1) # (n_routed_experts,) tokens_per_expert = cnts.sum(dim=0) # (b * s * num_experts_per_tok,) idxs = topk_ids.view(-1).argsort() # (b * s * num_experts_per_tok, hidden_size) sorted_tokens = x[idxs // topk_ids.shape[1]] sorted_tokens_shape = sorted_tokens.shape # (n_routed_experts,) tokens_per_expert = tokens_per_expert.cpu().numpy() outputs = [] start_idx = 0 for i, num_tokens in enumerate(tokens_per_expert): end_idx = start_idx + num_tokens if num_tokens == 0: continue expert = self.experts[i + self.ep_rank * self.experts_per_rank] tokens_for_this_expert = sorted_tokens[start_idx:end_idx] expert_out = expert(tokens_for_this_expert) outputs.append(expert_out) start_idx = end_idx outs = torch.cat(outputs, dim=0) if len(outputs) else sorted_tokens.new_empty(0) new_x = torch.empty_like(outs) new_x[idxs] = outs final_out = ( new_x.view(*topk_ids.shape, -1) .type(topk_weight.dtype) .mul_(topk_weight.unsqueeze(dim=-1)) .sum(dim=1) .type(new_x.dtype) ) return final_out 最后融合 shared experts 的输出.\n1 2 3 if self.config.n_shared_experts is not None: y = y + self.shared_experts(identity) return y 训练方案 Pre-Training 数据构建 预训练数据集构建的目标是尽量增强数据的多样性和丰富性. 整个数据处理过程分为三个阶段:\ndeduplication 去重, 增强数据质量 filtering 过滤, 保证信息密度 remixing 重新混合, 增强多样性 去重\n使用了激进的去重策略, 扩大了去重的范围. 使用 MinhashLSH 算法, 在 document 和 string 两个粒度下进行去重. String 粒度是将 document 进行切分, 原始的 document 可以看做是一个 dump, 将一个 document 切分为多个 dumps. 实际证明, 当切分为 91 个 dumps 的时候, 去重的比例是使用 document 粒度去重比例的 4 倍.\n严格去重策略, 保证了高质量数据的唯一性和完整性, 这在大规模数据集中特别重要.\n过滤\n过滤是对文本的质量进行评估, 这涉及到语言和语义的综合评估, 从单个样本, 以及整体样本的角度对数据质量进行评估, 移除低质量 web data. 对于高质量低资源的数据, 则不进行筛选.\n过滤的手段主要包含:\nheuristic rules, 启发式规则 models, 评估模型 通过各种手段, 移除有害有毒有争议的文本.\n重新混合\n在这个阶段, 通过数据的重新混合, 解决数据不平衡问题, 增加代表性不足领域的数据.\n混合了 Internet text, math, code, books, 以及自己采集的数据. 在多样性之外, 还特别重视个人隐私和著作权的保护, 将侵犯隐私和知识产权的内容从数据集中移除.\n总结\n训练集共收集了 8.1T tokens.\n训练超参数 初始化\n标准差 0.006\n优化器\nAdamW. $\\beta_1=0.9$, $\\beta_2=0.95$, $\\text{weight\\_decay} = 0.1$\nscheduler\n2000 steps warmup steps cosine scheduler: 训练到 60% 的 tokens 时, 学习率下降到最大值的 0.316; 到 90% 的 tokens 时, 再下降到 60% 位置对应学习率的 0.316 gradient clipping: 1.0 其他关键参数\nsequence length: 4096 learning rate: 2.4e-4 batch size: batch size scheduling strategy 在训练头 225B tokens 时, batch size 从 2304 逐渐增加到 9216 在剩余的训练过程中保持 9216 的大小 长上下文扩展 在预训练得到初始 4K 上下文长度版本的 DeepSeek-V2 后, 使用 YaRN 方法, 将上下文长度从 4K 扩展到 128K. 由于 MLA 中只有 $\\mathbf{k}_t^R$ 中带有 RoPE 的位置信息, 所以 YaRN 也是施加到这个上面. 使用的 YaRN 参数为:\nscale $s=40$ $\\alpha=1$ $\\beta=32$ target maximum context length = 160K 另外为了适配 MLA 机制, 调整了 length scaling factor, 因子 $\\sqrt{t} = 0.0707 \\ln s + 1$, 以最小化 perplexity.\nYaRN 的方法需要训练. 训练了 1000 steps, 使用了 batch size 576, sequence length 32K. 尽管训练时基于 32K 的序列长度, 在 128K 长度上已经可以取得比较 robust 的效果了.\n对齐训练 SFT 1.5M instances 的 instruction tuning datasets, 由 1.2M helpfulness instances 和 0.3M safety instances 组成. 加强了数据质量, 以减轻幻觉, 以及增强写作能力.\nSFT with 2 epochs learning rate 5e-6 RL DeepSeek-V2 没有使用 RM + PPO, 而是使用了 RM + GRPO 的方法. 相比之下, 这种方法不需要更新 policy 中的 value model 的方法.\n参考资料 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model DeepSeek LLM Scaling Open-Source Language Models with Longtermism Github DeepSeek-LLM ","date":"2024-05-13T10:28:37+08:00","permalink":"http://localhost:1313/p/deepseek-v2-insight/","title":"Deepseek V2 Insight"}]